{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ec0d83-31c3-4e4b-9298-2c2c666b1412",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enable MLflow Tracing for GenAI Development"
    }
   },
   "outputs": [],
   "source": [
    "###  It is highly recommended that you enable MLflow Tracing when developing GenAI applications. Tracing provides a step-by-step analysis of your app's execution, helping you debug latency, cost, and quality issues. Traces can be used with Agent Evaluation to measure your app's quality, and the same Trace will be automatically captured once you deploy your application.\n",
    "\n",
    "\n",
    "%pip install -U mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257a261f-11e6-47ad-9f6b-0be1a64d13a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Autologging OpenAI with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.openai.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9607211b-19fd-4a71-bc61-a595a9acda49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve OpenAI response with Databricks token"
    }
   },
   "outputs": [],
   "source": "from databricks.sdk import WorkspaceClient\n\n# Initialize workspace client using notebook's default authentication\n# No manual PAT or OpenAI API key required - uses Databricks notebook context\nworkspace_client = WorkspaceClient()\n\n# Get OpenAI-compatible client for Databricks serving endpoints\nclient = workspace_client.serving_endpoints.get_open_ai_client()\n\n# Define the LLM endpoint to use\nLLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n\nresponse = client.responses.create(\n    model=\"mas-01875d0e-endpoint\",\n    input=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What are the emerging investment themes mentioned in the market research documents?\"\n        }\n    ]\n)\n\nprint(response.output[0].content[0].text)"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "OpenAI LLM Agent Inquiry",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}