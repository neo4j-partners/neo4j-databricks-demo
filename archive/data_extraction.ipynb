{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a3098a-0d60-44fe-85a1-3ce208e91e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Neo4j to Delta Lake: Spark Connector Data Extraction\n",
    "\n",
    "This notebook demonstrates a **simplified approach** to extracting graph data from Neo4j into Databricks Delta Lake tables using the **Neo4j Spark Connector**.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "### What This Notebook Does\n",
    "Extracts **node data** from Neo4j using the Neo4j Spark Connector and creates separate Delta tables for each entity type:\n",
    "- üë• **Customer** - Customer profile data\n",
    "- üè¶ **Bank** - Financial institution data\n",
    "- üíº **Account** - Customer account information\n",
    "- üè¢ **Company** - Corporate entity information\n",
    "- üìà **Stock** - Stock and security data\n",
    "- üìä **Position** - Investment portfolio holdings\n",
    "- üí∞ **Transaction** - Financial transaction records\n",
    "\n",
    "### Key Features\n",
    "- ‚úÖ **Direct Spark Pipeline** - Neo4j ‚Üí Spark ‚Üí Delta (no intermediate conversions)\n",
    "- ‚úÖ **Type Safety** - Fixed Spark schemas using `neo4j_schemas.py` module\n",
    "- ‚úÖ **Simple Code** - No custom config modules or helper functions\n",
    "- ‚úÖ **Metadata Tracking** - Includes node ID, labels, and ingestion timestamp\n",
    "\n",
    "### Unity Catalog Location\n",
    "This notebook writes to: `retail_investment.retail_investment.*`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9e1ba4-17a7-46d1-9dea-f550195e254f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup: Import Schema Module\n",
    "\n",
    "Import the `neo4j_schemas` module which provides fixed Spark schemas for all node types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab8d1e5-2a50-40a0-90ea-46dd1ffafabe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ship Schemas ..."
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "\n",
    "# Configure Python path for schema module\n",
    "python_repo_url = \"/Workspace/Users/ryan.knight@neo4j.com/neo4j-databricks-demo\"\n",
    "sys.path.append(f\"{python_repo_url}/neo4j_schemas.py\")\n",
    "sys.path.append(f\"{python_repo_url}/databricks_constants.py\")\n",
    "\n",
    "# Import schema module - platform-agnostic schemas\n",
    "from neo4j_schemas import (\n",
    "    get_node_schema,\n",
    "    get_version,\n",
    "    list_node_schemas,\n",
    "    validate_node_schema,\n",
    "    # Relationship schema imports\n",
    "    get_relationship_schema,\n",
    "    list_relationship_schemas,\n",
    "    RELATIONSHIP_METADATA,\n",
    "    get_relationship_metadata,\n",
    "    validate_relationship_schema,\n",
    ")\n",
    "\n",
    "# Import Databricks-specific constants\n",
    "from databricks_constants import (\n",
    "    NODE_TABLE_NAMES,\n",
    "    RELATIONSHIP_TABLE_NAMES,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Schema module loaded (version {get_version()})\")\n",
    "print(f\"\\nAvailable node schemas: {', '.join(list_node_schemas())}\")\n",
    "print(f\"Available relationship schemas: {', '.join(list_relationship_schemas())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0537652-4a8f-417f-8050-d7d79b79066d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Tables in Schema"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Optional - List all tables in the schema\n",
    "tables = spark.sql(\"SHOW TABLES IN retail_investment.retail_investment\").toPandas()\n",
    "\n",
    "# Generate and execute DROP TABLE statements\n",
    "for table in tables['tableName']:\n",
    "    drop_stmt = f\"DROP TABLE retail_investment.retail_investment.{table}\"\n",
    "    print(f\"Executing: {drop_stmt}\")\n",
    "    spark.sql(drop_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16fdab3-8bc6-462e-ae8b-41ade2894855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Neo4j Spark Connector\n",
    "\n",
    "Configure connection to Neo4j using environment variables and Databricks secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe91f75-be0f-435b-b973-960c9c40cffa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Neo4j Connector Configuration Check"
    }
   },
   "outputs": [],
   "source": [
    "# Get Neo4j connection details from environment variables\n",
    "neo4j_username = \"neo4j\"\n",
    "neo4j_database = \"neo4j\"\n",
    "\n",
    "# Get password from Databricks secrets\n",
    "neo4j_password = dbutils.secrets.get(scope=\"neo4j\", key=\"password\")\n",
    "neo4j_url = dbutils.secrets.get(scope=\"neo4j\", key=\"neo4j_url\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([neo4j_url, neo4j_username, neo4j_database, neo4j_password]):\n",
    "    raise ValueError(\"Missing Neo4j configuration. Please check environment variables and secrets.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Neo4j Spark Connector Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL:      {neo4j_url}\")\n",
    "print(f\"Username: {neo4j_username}\")\n",
    "print(f\"Database: {neo4j_database}\")\n",
    "print(f\"Password: {'*' * len(neo4j_password)}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Configuration validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60e9a9a-7f66-4d65-a824-981a395e33a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Extraction Function\n",
    "\n",
    "Define the function that extracts nodes from Neo4j to Delta Lake using the **optimized `labels` option approach**.\n",
    "\n",
    "This approach:\n",
    "- ‚úÖ Automatically includes all node properties\n",
    "- ‚úÖ Includes metadata (`<id>` and `<labels>` columns)\n",
    "- ‚úÖ No manual field listing required\n",
    "- ‚úÖ Simple, clean code for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c277a5-5d70-420e-ac75-5b21a9bee78a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "onnector ..."
    }
   },
   "outputs": [],
   "source": [
    "def extract_node_type_to_delta(node_label: str, limit: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract nodes from Neo4j using Spark Connector and write to Delta table.\n",
    "    \n",
    "    IMPLEMENTATION: Uses option(\"labels\", ...) approach (BEST PRACTICE)\n",
    "    - Automatically includes ALL node properties\n",
    "    - Automatically includes <id> and <labels> metadata\n",
    "    - Simple, clean code for demo purposes\n",
    "    \n",
    "    Args:\n",
    "        node_label (str): Neo4j node label (e.g., 'Customer', 'Bank')\n",
    "        limit (int): Maximum number of records to extract (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Extraction statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Extracting {node_label} nodes...\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    try:\n",
    "        # Read from Neo4j using labels option\n",
    "        # This automatically includes:\n",
    "        # - <id>: Neo4j internal node ID (long)\n",
    "        # - <labels>: Array of node labels\n",
    "        # - All node properties\n",
    "        df = (spark.read\n",
    "              .format(\"org.neo4j.spark.DataSource\")\n",
    "              .option(\"url\", neo4j_url)\n",
    "              .option(\"authentication.type\", \"basic\")\n",
    "              .option(\"authentication.basic.username\", neo4j_username)\n",
    "              .option(\"authentication.basic.password\", neo4j_password)\n",
    "              .option(\"database\", neo4j_database)\n",
    "              .option(\"labels\", node_label)\n",
    "              .load())\n",
    "        \n",
    "        # Rename metadata columns to match schema module expectations\n",
    "        # <id> ‚Üí neo4j_id, <labels> ‚Üí neo4j_labels\n",
    "        # and add ingestion timestamp\n",
    "        df_final = (df\n",
    "                    .withColumnRenamed(\"<id>\", \"neo4j_id\")\n",
    "                    .withColumnRenamed(\"<labels>\", \"neo4j_labels\")\n",
    "                    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "                    .limit(limit))\n",
    "        \n",
    "        # Get table name from schema module\n",
    "        table_name = NODE_TABLE_NAMES[node_label]\n",
    "        \n",
    "        # Write to Delta table\n",
    "        print(f\"  ‚öôÔ∏è  Writing to Delta table...\")\n",
    "        (df_final.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .saveAsTable(table_name))\n",
    "        \n",
    "        # Get record count\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").first()[\"count\"]\n",
    "        \n",
    "        print(f\"  ‚úÖ Complete: {count} records ‚Üí {table_name}\")\n",
    "        \n",
    "        return {\n",
    "            \"node_label\": node_label,\n",
    "            \"record_count\": count,\n",
    "            \"table_name\": table_name,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR: {str(e)}\")\n",
    "        return {\n",
    "            \"node_label\": node_label,\n",
    "            \"record_count\": 0,\n",
    "            \"table_name\": NODE_TABLE_NAMES.get(node_label, \"unknown\"),\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Extraction function defined with simplified approach for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9401683-e92d-4c2a-bc8c-85bfcd0873c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extraction Summary\n",
    "\n",
    "Display statistics about the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d84d61-8578-4234-866d-6bed5c896553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTRACTING NODE DATA FROM NEO4J TO DELTA TABLES\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Define node types to extract - aligned with neo4j_schemas.py definitions\n",
    "node_types = [\n",
    "    \"Account\",\n",
    "    \"Bank\",\n",
    "    \"Company\",\n",
    "    \"Customer\",\n",
    "    \"Position\",\n",
    "    \"Stock\",\n",
    "    \"Transaction\",\n",
    "]\n",
    "\n",
    "# Track extraction statistics with enhanced details\n",
    "extraction_stats = {}\n",
    "\n",
    "# Extract each node type\n",
    "for node_label in node_types:\n",
    "    stats = extract_node_type_to_delta(node_label, limit=100)\n",
    "    extraction_stats[node_label] = stats\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41dbd683-15c0-41de-97bf-2e7952eb54c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create summary DataFrame with extraction results\n",
    "summary_data = []\n",
    "for node_type in node_types:\n",
    "    stats = extraction_stats[node_type]\n",
    "    \n",
    "    # Determine status display\n",
    "    if stats[\"status\"] == \"error\":\n",
    "        status_display = \"‚ùå Error\"\n",
    "        details = stats.get(\"error\", \"Unknown error\")\n",
    "    elif stats[\"record_count\"] == 0:\n",
    "        status_display = \"‚ö†Ô∏è  Empty\"\n",
    "        details = \"No records found\"\n",
    "    else:\n",
    "        status_display = \"‚úÖ Success\"\n",
    "        details = \"OK\"\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"Node Type\": node_type,\n",
    "        \"Records\": stats[\"record_count\"],\n",
    "        \"Delta Table\": stats[\"table_name\"],\n",
    "        \"Status\": status_display,\n",
    "        \"Details\": details\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"NODE EXTRACTION SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"Total Tables Created: {len(extraction_stats)}\")\n",
    "print(f\"Total Records Extracted: {sum(s['record_count'] for s in extraction_stats.values())}\")\n",
    "print(f\"Successful Extractions: {sum(1 for s in extraction_stats.values() if s['status'] == 'success')}\")\n",
    "print(f\"Catalog Location: retail_investment.retail_investment\")\n",
    "print(f\"Extraction Method: Neo4j Spark Connector (org.neo4j.spark.DataSource)\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27f6521-ff22-4449-9ec0-83f4522e3423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data Preview\n",
    "\n",
    "Preview sample records from each extracted node type to verify data quality and schema correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf591b9-89c9-4da3-9aa2-c4648d316edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display sample data from each node type for verification\n",
    "for node_label in node_types:\n",
    "    stats = extraction_stats[node_label]\n",
    "    table_name = stats[\"table_name\"]\n",
    "    \n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"Sample Data: {node_label} ({stats['record_count']} total records)\")\n",
    "    print(f\"Table: {table_name}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    if stats[\"record_count\"] > 0:\n",
    "        # Show schema\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {table_name} LIMIT 3\")\n",
    "        print(f\"\\nSchema:\")\n",
    "        sample_df.printSchema()\n",
    "        \n",
    "        print(f\"\\nSample Records (3 of {stats['record_count']}):\")\n",
    "        display(sample_df)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No records to display\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb42e63-6bcb-4964-86d7-8513c218cd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Detailed Schema Inspection\n",
    "\n",
    "Examine the detailed schema and metadata for the Customer table to verify field types, constraints, and Delta table properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f90999-3fc8-41b0-b15c-12eecdcbb69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show detailed schema for Customer table as an example\n",
    "example_table = NODE_TABLE_NAMES[\"Customer\"]\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(f\"Detailed Schema Inspection: Customer Table\")\n",
    "print(f\"Table: {example_table}\")\n",
    "print(f\"{'=' * 100}\\n\")\n",
    "\n",
    "# Show schema in tree format\n",
    "customer_df = spark.table(example_table)\n",
    "print(\"Schema Structure:\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"Extended Table Metadata:\")\n",
    "print(\"=\" * 100)\n",
    "display(spark.sql(f\"DESCRIBE TABLE EXTENDED {example_table}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d9ebeb-0045-4492-82ce-3fcbed443366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## PART 2: Relationship (Edge) Extraction\n",
    "\n",
    "Extract relationship data from Neo4j to create graph edge tables in Delta Lake. This enables graph analytics, path queries, and network analysis using standard SQL and Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7f408d-2f31-4c56-8d09-8d7021174118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Relationship Extraction Function\n",
    "\n",
    "Define the function that extracts relationships (edges) from Neo4j to Delta Lake using the **`relationship` option approach**.\n",
    "\n",
    "This approach:\n",
    "- ‚úÖ Automatically includes relationship metadata (`<rel.id>`, `<rel.type>`, source/target IDs)\n",
    "- ‚úÖ Uses business-meaningful column names (e.g., `customerId`, `senderAccountId`)\n",
    "- ‚úÖ Simple, clean code following best practices from RELATIONSHIP_HANDLING.md\n",
    "- ‚úÖ Ready for graph analytics and path queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f47657-12ed-4006-a960-6f9a9b73b23f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_relationship_type_to_delta(rel_type: str, limit: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract relationships from Neo4j using Spark Connector and write to Delta table.\n",
    "    \n",
    "    Args:\n",
    "        rel_type (str): Neo4j relationship type (e.g., 'HAS_ACCOUNT', 'PERFORMS')\n",
    "        limit (int): Maximum number of records to extract (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Extraction statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Extracting {rel_type} relationships...\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    try:\n",
    "        # Get metadata for this relationship type\n",
    "        metadata = get_relationship_metadata(rel_type)\n",
    "        \n",
    "        source_label = metadata[\"source_label\"]\n",
    "        dest_label = metadata[\"destination_label\"]\n",
    "        source_key = metadata[\"source_key\"]  # Actual property name on source node\n",
    "        dest_key = metadata[\"destination_key\"]  # Actual property name on target node\n",
    "        \n",
    "        # Get schema to determine output column names\n",
    "        from neo4j_schemas import BASE_RELATIONSHIP_SCHEMAS\n",
    "        schema = BASE_RELATIONSHIP_SCHEMAS[rel_type]\n",
    "        output_source_col = schema.fields[0].name  # First field is source key\n",
    "        output_dest_col = schema.fields[1].name    # Second field is destination key\n",
    "        \n",
    "        print(f\"  ‚öôÔ∏è  Pattern: ({source_label})-[:{rel_type}]->({dest_label})\")\n",
    "        print(f\"  ‚öôÔ∏è  Neo4j Keys: {source_key} ‚Üí {dest_key}\")\n",
    "        print(f\"  ‚öôÔ∏è  Output Columns: {output_source_col} ‚Üí {output_dest_col}\")\n",
    "        \n",
    "        # Read from Neo4j using relationship option in FLAT MODE\n",
    "        df = (spark.read\n",
    "              .format(\"org.neo4j.spark.DataSource\")\n",
    "              .option(\"url\", neo4j_url)\n",
    "              .option(\"authentication.type\", \"basic\")\n",
    "              .option(\"authentication.basic.username\", neo4j_username)\n",
    "              .option(\"authentication.basic.password\", neo4j_password)\n",
    "              .option(\"database\", neo4j_database)\n",
    "              .option(\"relationship\", rel_type)\n",
    "              .option(\"relationship.source.labels\", source_label)\n",
    "              .option(\"relationship.target.labels\", dest_label)\n",
    "              .option(\"relationship.nodes.map\", \"false\")\n",
    "              .load())\n",
    "        \n",
    "        # Build column selection using actual Neo4j property names\n",
    "        source_col = f\"`source.{source_key}`\"\n",
    "        dest_col = f\"`target.{dest_key}`\"\n",
    "        \n",
    "        # Select and rename columns to match schema expectations\n",
    "        df_final = (df\n",
    "                    .select(\n",
    "                        col(source_col).alias(output_source_col),\n",
    "                        col(dest_col).alias(output_dest_col),\n",
    "                        col(\"`<rel.id>`\").alias(\"rel_element_id\"),\n",
    "                        col(\"`<rel.type>`\").alias(\"rel_type\"),\n",
    "                        col(\"`<source.id>`\").alias(\"src_neo4j_id\"),\n",
    "                        col(\"`<target.id>`\").alias(\"dst_neo4j_id\")\n",
    "                    )\n",
    "                    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "                    .limit(limit))\n",
    "        \n",
    "        # Get table name from schema module\n",
    "        table_name = RELATIONSHIP_TABLE_NAMES[rel_type]\n",
    "        \n",
    "        # Write to Delta table\n",
    "        print(f\"  ‚öôÔ∏è  Writing to Delta table...\")\n",
    "        (df_final.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .saveAsTable(table_name))\n",
    "        \n",
    "        # Get record count\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").first()[\"count\"]\n",
    "        \n",
    "        print(f\"  ‚úÖ Complete: {count} edges ‚Üí {table_name}\")\n",
    "        \n",
    "        return {\n",
    "            \"rel_type\": rel_type,\n",
    "            \"record_count\": count,\n",
    "            \"table_name\": table_name,\n",
    "            \"pattern\": f\"{source_label} ‚Üí {dest_label}\",\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR: {str(e)}\")\n",
    "        return {\n",
    "            \"rel_type\": rel_type,\n",
    "            \"record_count\": 0,\n",
    "            \"table_name\": RELATIONSHIP_TABLE_NAMES.get(rel_type, \"unknown\"),\n",
    "            \"pattern\": \"unknown\",\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Relationship extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44da2f9-9eb8-43f0-af79-4247ec85e2fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract All Relationship Types\n",
    "\n",
    "Extract all 7 relationship types from Neo4j to Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5123f7cc-71b5-4066-ae52-b71b114575f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTRACTING RELATIONSHIP DATA FROM NEO4J TO DELTA TABLES\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Define relationship types to extract - aligned with neo4j_schemas.py definitions\n",
    "relationship_types = [\n",
    "    \"HAS_ACCOUNT\",\n",
    "    \"AT_BANK\",\n",
    "    \"OF_COMPANY\",\n",
    "    \"PERFORMS\",\n",
    "    \"BENEFITS_TO\",\n",
    "    \"HAS_POSITION\",\n",
    "    \"OF_SECURITY\",\n",
    "]\n",
    "\n",
    "# Track extraction statistics\n",
    "relationship_stats = {}\n",
    "\n",
    "# Extract each relationship type\n",
    "for rel_type in relationship_types:\n",
    "    stats = extract_relationship_type_to_delta(rel_type, limit=100)\n",
    "    relationship_stats[rel_type] = stats\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ RELATIONSHIP EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be46b0d8-ad16-4e77-942c-0bb0269327bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Relationship Extraction Summary\n",
    "\n",
    "Display statistics about the relationship extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39baeb79-d75a-40d7-b980-25b858047c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create summary DataFrame with relationship extraction results\n",
    "rel_summary_data = []\n",
    "for rel_type in relationship_types:\n",
    "    stats = relationship_stats[rel_type]\n",
    "    \n",
    "    # Determine status display\n",
    "    if stats[\"status\"] == \"error\":\n",
    "        status_display = \"‚ùå Error\"\n",
    "        error_msg = stats.get(\"error\", \"Unknown error\")\n",
    "    elif stats[\"record_count\"] == 0:\n",
    "        status_display = \"‚ö†Ô∏è  Empty\"\n",
    "        error_msg = \"No records found\"\n",
    "    else:\n",
    "        status_display = \"‚úÖ Success\"\n",
    "        error_msg = \"OK\"\n",
    "    \n",
    "    rel_summary_data.append({\n",
    "        \"Relationship\": rel_type,\n",
    "        \"Pattern\": stats[\"pattern\"],\n",
    "        \"Records\": stats[\"record_count\"],\n",
    "        \"Delta Table\": stats[\"table_name\"],\n",
    "        \"Status\": status_display,\n",
    "        \"Details\": error_msg if stats[\"status\"] != \"success\" else \"OK\"\n",
    "    })\n",
    "\n",
    "rel_summary_df = pd.DataFrame(rel_summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"RELATIONSHIP EXTRACTION SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(rel_summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"Total Edge Tables Created: {len(relationship_stats)}\")\n",
    "print(f\"Total Edges Extracted: {sum(s['record_count'] for s in relationship_stats.values())}\")\n",
    "print(f\"Successful Extractions: {sum(1 for s in relationship_stats.values() if s['status'] == 'success')}\")\n",
    "print(f\"Catalog Location: retail_investment.retail_investment\")\n",
    "print(f\"Extraction Method: Neo4j Spark Connector (org.neo4j.spark.DataSource) with relationship option\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aaa1717-8aa9-41c7-a1a7-43d8d11daac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Relationship Queries\n",
    "\n",
    "Demonstrate how to use the relationship tables with joins to perform graph analytics using standard SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e19615-9411-4d07-925f-a17462c08f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Customer Accounts with Bank Information\n",
    "# Join: Customer -[HAS_ACCOUNT]-> Account -[AT_BANK]-> Bank\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Example 1: Customer Accounts with Bank Information\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    c.customerId,\n",
    "    c.firstName,\n",
    "    c.lastName,\n",
    "    a.accountId,\n",
    "    a.accountType,\n",
    "    a.balance,\n",
    "    b.name AS bank_name,\n",
    "    b.bankType\n",
    "FROM retail_investment.retail_investment.customer c\n",
    "JOIN retail_investment.retail_investment.has_account ha ON c.customerId = ha.customerId\n",
    "JOIN retail_investment.retail_investment.account a ON ha.accountId = a.accountId\n",
    "JOIN retail_investment.retail_investment.at_bank ab ON a.accountId = ab.accountId\n",
    "JOIN retail_investment.retail_investment.bank b ON ab.bankId = b.bankId\n",
    "ORDER BY a.balance DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result1 = spark.sql(query1)\n",
    "print(\"\\nTop 10 Accounts by Balance with Customer and Bank Information:\")\n",
    "display(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb773e4-79d5-47b9-901a-1256f9b6b321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Portfolio Holdings Analysis\n",
    "# Join: Account -[HAS_POSITION]-> Position -[OF_SECURITY]-> Stock -[OF_COMPANY]-> Company\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 3: Portfolio Holdings with Stock and Company Details\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    hp.accountId,\n",
    "    p.positionId,\n",
    "    p.shares,\n",
    "    p.currentValue,\n",
    "    p.percentageOfPortfolio,\n",
    "    s.ticker,\n",
    "    s.currentPrice,\n",
    "    co.name AS company_name,\n",
    "    co.sector,\n",
    "    co.industry\n",
    "FROM retail_investment.retail_investment.has_position hp\n",
    "JOIN retail_investment.retail_investment.position p ON hp.positionId = p.positionId\n",
    "JOIN retail_investment.retail_investment.of_security os ON p.positionId = os.positionId\n",
    "JOIN retail_investment.retail_investment.stock s ON os.stockId = s.stockId\n",
    "JOIN retail_investment.retail_investment.of_company oc ON s.stockId = oc.stockId\n",
    "JOIN retail_investment.retail_investment.company co ON oc.companyId = co.companyId\n",
    "ORDER BY p.currentValue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result3 = spark.sql(query3)\n",
    "print(\"\\nTop 10 Portfolio Positions by Value:\")\n",
    "display(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdba27d-d824-46b0-b725-b53238f6c26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 3: Complete Customer Financial Profile\n",
    "# Multi-path join showing customer's accounts, transactions, and investments\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 4: Complete Financial Profile for a Sample Customer\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "query4 = \"\"\"\n",
    "WITH customer_accounts AS (\n",
    "    SELECT \n",
    "        c.customerId,\n",
    "        c.firstName,\n",
    "        c.lastName,\n",
    "        c.riskProfile,\n",
    "        a.accountId,\n",
    "        a.accountType,\n",
    "        a.balance\n",
    "    FROM retail_investment.retail_investment.customer c\n",
    "    JOIN retail_investment.retail_investment.has_account ha ON c.customerId = ha.customerId\n",
    "    JOIN retail_investment.retail_investment.account a ON ha.accountId = a.accountId\n",
    "),\n",
    "account_positions AS (\n",
    "    SELECT \n",
    "        hp.accountId,\n",
    "        COUNT(*) AS num_positions,\n",
    "        SUM(p.currentValue) AS total_portfolio_value\n",
    "    FROM retail_investment.retail_investment.has_position hp\n",
    "    JOIN retail_investment.retail_investment.position p ON hp.positionId = p.positionId\n",
    "    GROUP BY hp.accountId\n",
    ")\n",
    "/*\n",
    ",account_transactions AS (\n",
    "    SELECT\n",
    "        p.senderAccountId AS accountId,\n",
    "        COUNT(*) AS num_transactions,\n",
    "        SUM(t.amount) AS total_sent\n",
    "    FROM retail_investment.retail_investment.performs p\n",
    "    JOIN retail_investment.retail_investment.transaction t ON p.transactionId = t.transactionId\n",
    "    GROUP BY p.senderAccountId\n",
    ")\n",
    "*/\n",
    "SELECT\n",
    "    ca.customerId,\n",
    "    ca.firstName,\n",
    "    ca.lastName,\n",
    "    ca.riskProfile,\n",
    "    ca.accountId,\n",
    "    ca.accountType,\n",
    "    ca.balance,\n",
    "    COALESCE(ap.num_positions, 0) AS num_positions,\n",
    "    COALESCE(ap.total_portfolio_value, 0) AS portfolio_value\n",
    "    -- ,COALESCE(at.num_transactions, 0) AS num_transactions_sent\n",
    "    -- ,COALESCE(at.total_sent, 0) AS total_amount_sent\n",
    "FROM customer_accounts ca\n",
    "LEFT JOIN account_positions ap ON ca.accountId = ap.accountId\n",
    "-- LEFT JOIN account_transactions at ON ca.accountId = at.accountId\n",
    "ORDER BY ca.balance DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result4 = spark.sql(query4)\n",
    "print(\"\\nComplete Financial Profile (Top 10 Customers by Account Balance):\")\n",
    "display(result4)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_extraction",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
