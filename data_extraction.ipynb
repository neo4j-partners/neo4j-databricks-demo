{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Neo4j to Delta Lake: Spark Connector Data Extraction\n\nThis notebook demonstrates a **simplified approach** to extracting graph data from Neo4j into Databricks Delta Lake tables using the **Neo4j Spark Connector**.\n\n---\n\n## Overview\n\n### What This Notebook Does\nExtracts **node data** from Neo4j using the Neo4j Spark Connector and creates separate Delta tables for each entity type:\n- üë• **Customer** - Customer profile data\n- üè¶ **Bank** - Financial institution data\n- üíº **Account** - Customer account information\n- üè¢ **Company** - Corporate entity information\n- üìà **Stock** - Stock and security data\n- üìä **Position** - Investment portfolio holdings\n- üí∞ **Transaction** - Financial transaction records\n\n### Key Features\n- ‚úÖ **Direct Spark Pipeline** - Neo4j ‚Üí Spark ‚Üí Delta (no intermediate conversions)\n- ‚úÖ **Type Safety** - Fixed Spark schemas using `neo4j_schemas.py` module\n- ‚úÖ **Simple Code** - No custom config modules or helper functions\n- ‚úÖ **Metadata Tracking** - Includes node ID, labels, and ingestion timestamp\n\n### Unity Catalog Location\nThis notebook writes to: `fintech.default.neo4j_databricks_graph_demo_*`\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup: Import Schema Module\n",
    "\n",
    "Import the `neo4j_schemas` module which provides fixed Spark schemas for all node types."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Standard library imports\nimport os\nimport sys\nimport traceback\nfrom typing import Dict, Any\n\n# Third-party imports\nimport pandas as pd\nfrom pyspark.sql.functions import col, current_timestamp, lit\n\n# Configure Python path for schema module\npython_repo_url = os.environ.get(\"PYTHON_REPO_URL\")\nsys.path.append(f\"{python_repo_url}/neo4j_schemas.py\")\nsys.path.append(f\"{python_repo_url}/databricks_constants.py\")\n\n# Import schema module - platform-agnostic schemas\nfrom neo4j_schemas import (\n    get_node_schema,\n    get_version,\n    list_node_schemas,\n    validate_node_schema,\n    # Relationship schema imports\n    get_relationship_schema,\n    list_relationship_schemas,\n    RELATIONSHIP_METADATA,\n    get_relationship_metadata,\n    validate_relationship_schema,\n)\n\n# Import Databricks-specific constants\nfrom databricks_constants import (\n    NODE_TABLE_NAMES,\n    RELATIONSHIP_TABLE_NAMES,\n)\n\nprint(f\"‚úÖ Schema module loaded (version {get_version()})\")\nprint(f\"\\nAvailable node schemas: {', '.join(list_node_schemas())}\")\nprint(f\"Available relationship schemas: {', '.join(list_relationship_schemas())}\")",
   "id": "6af0f7b7a61216c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Optional - List all tables in the schema\n",
    "tables = spark.sql(\"SHOW TABLES IN fintech.default\").toPandas()\n",
    "\n",
    "# Generate and execute DROP TABLE statements\n",
    "for table in tables['tableName']:\n",
    "    drop_stmt = f\"DROP TABLE fintech.default.{table}\"\n",
    "    print(f\"Executing: {drop_stmt}\")\n",
    "    spark.sql(drop_stmt)"
   ],
   "id": "bdee6f833eecd85d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configure Neo4j Spark Connector\n",
    "\n",
    "Configure connection to Neo4j using environment variables and Databricks secrets."
   ],
   "id": "6e4f2956e0a89fe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get Neo4j connection details from environment variables\n",
    "neo4j_url = os.environ.get(\"NEO4J_URL\")\n",
    "neo4j_username = os.environ.get(\"NEO4J_USERNAME\")\n",
    "neo4j_database = os.environ.get(\"NEO4J_DATABASE\")\n",
    "\n",
    "# Get password from Databricks secrets\n",
    "neo4j_password = dbutils.secrets.get(scope=\"neo4j\", key=\"password\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([neo4j_url, neo4j_username, neo4j_database, neo4j_password]):\n",
    "    raise ValueError(\"Missing Neo4j configuration. Please check environment variables and secrets.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Neo4j Spark Connector Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL:      {neo4j_url}\")\n",
    "print(f\"Username: {neo4j_username}\")\n",
    "print(f\"Database: {neo4j_database}\")\n",
    "print(f\"Password: {'*' * len(neo4j_password)}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Configuration validated\")"
   ],
   "id": "74f9386ba057cd85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3f35n2noba",
   "source": "## Define Extraction Function\n\nDefine the function that extracts nodes from Neo4j to Delta Lake using the **optimized `labels` option approach**.\n\nThis approach:\n- ‚úÖ Automatically includes all node properties\n- ‚úÖ Includes metadata (`<id>` and `<labels>` columns)\n- ‚úÖ No manual field listing required\n- ‚úÖ Simple, clean code for demo purposes",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def extract_node_type_to_delta(node_label: str, limit: int = 100) -> Dict[str, Any]:\n    \"\"\"\n    Extract nodes from Neo4j using Spark Connector and write to Delta table.\n    \n    IMPLEMENTATION: Uses option(\"labels\", ...) approach (BEST PRACTICE)\n    - Automatically includes ALL node properties\n    - Automatically includes <id> and <labels> metadata\n    - Simple, clean code for demo purposes\n    \n    Args:\n        node_label (str): Neo4j node label (e.g., 'Customer', 'Bank')\n        limit (int): Maximum number of records to extract (default: 100)\n    \n    Returns:\n        Dict[str, Any]: Extraction statistics\n    \"\"\"\n    print(f\"\\n{'=' * 70}\")\n    print(f\"Extracting {node_label} nodes...\")\n    print(f\"{'=' * 70}\")\n    \n    try:\n        # Read from Neo4j using labels option\n        # This automatically includes:\n        # - <id>: Neo4j internal node ID (long)\n        # - <labels>: Array of node labels\n        # - All node properties\n        df = (spark.read\n              .format(\"org.neo4j.spark.DataSource\")\n              .option(\"url\", neo4j_url)\n              .option(\"authentication.type\", \"basic\")\n              .option(\"authentication.basic.username\", neo4j_username)\n              .option(\"authentication.basic.password\", neo4j_password)\n              .option(\"database\", neo4j_database)\n              .option(\"labels\", node_label)\n              .load())\n        \n        # Rename metadata columns to match schema module expectations\n        # <id> ‚Üí neo4j_id, <labels> ‚Üí neo4j_labels\n        # and add ingestion timestamp\n        df_final = (df\n                    .withColumnRenamed(\"<id>\", \"neo4j_id\")\n                    .withColumnRenamed(\"<labels>\", \"neo4j_labels\")\n                    .withColumn(\"ingestion_timestamp\", current_timestamp())\n                    .limit(limit))\n        \n        # Get table name from schema module\n        table_name = NODE_TABLE_NAMES[node_label]\n        \n        # Write to Delta table\n        print(f\"  ‚öôÔ∏è  Writing to Delta table...\")\n        (df_final.write\n         .format(\"delta\")\n         .mode(\"overwrite\")\n         .option(\"overwriteSchema\", \"true\")\n         .saveAsTable(table_name))\n        \n        # Get record count\n        count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").first()[\"count\"]\n        \n        print(f\"  ‚úÖ Complete: {count} records ‚Üí {table_name}\")\n        \n        return {\n            \"node_label\": node_label,\n            \"record_count\": count,\n            \"table_name\": table_name,\n            \"status\": \"success\"\n        }\n        \n    except Exception as e:\n        print(f\"  ‚ùå ERROR: {str(e)}\")\n        return {\n            \"node_label\": node_label,\n            \"record_count\": 0,\n            \"table_name\": NODE_TABLE_NAMES.get(node_label, \"unknown\"),\n            \"status\": \"error\",\n            \"error\": str(e)\n        }\n\nprint(\"‚úÖ Extraction function defined with simplified approach for demo\")",
   "id": "5c5741c1e3135097",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Extraction Summary\n\nDisplay statistics about the extraction process."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTRACTING NODE DATA FROM NEO4J TO DELTA TABLES\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Define node types to extract - aligned with neo4j_schemas.py definitions\n",
    "node_types = [\n",
    "    \"Account\",\n",
    "    \"Bank\",\n",
    "    \"Company\",\n",
    "    \"Customer\",\n",
    "    \"Position\",\n",
    "    \"Stock\",\n",
    "    \"Transaction\",\n",
    "]\n",
    "\n",
    "# Track extraction statistics with enhanced details\n",
    "extraction_stats = {}\n",
    "\n",
    "# Extract each node type\n",
    "for node_label in node_types:\n",
    "    stats = extract_node_type_to_delta(node_label, limit=100)\n",
    "    extraction_stats[node_label] = stats\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "e9077a84ae016704"
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {},
   "source": "# Create summary DataFrame with extraction results\nsummary_data = []\nfor node_type in node_types:\n    stats = extraction_stats[node_type]\n    \n    # Determine status display\n    if stats[\"status\"] == \"error\":\n        status_display = \"‚ùå Error\"\n        details = stats.get(\"error\", \"Unknown error\")\n    elif stats[\"record_count\"] == 0:\n        status_display = \"‚ö†Ô∏è  Empty\"\n        details = \"No records found\"\n    else:\n        status_display = \"‚úÖ Success\"\n        details = \"OK\"\n    \n    summary_data.append({\n        \"Node Type\": node_type,\n        \"Records\": stats[\"record_count\"],\n        \"Delta Table\": stats[\"table_name\"],\n        \"Status\": status_display,\n        \"Details\": details\n    })\n\nsummary_df = pd.DataFrame(summary_data)\n\nprint(\"\\n\" + \"=\" * 100)\nprint(\"NODE EXTRACTION SUMMARY\")\nprint(\"=\" * 100)\nprint(summary_df.to_string(index=False))\nprint(\"\\n\" + \"=\" * 100)\nprint(f\"Total Tables Created: {len(extraction_stats)}\")\nprint(f\"Total Records Extracted: {sum(s['record_count'] for s in extraction_stats.values())}\")\nprint(f\"Successful Extractions: {sum(1 for s in extraction_stats.values() if s['status'] == 'success')}\")\nprint(f\"Catalog Location: fintech.default\")\nprint(f\"Extraction Method: Neo4j Spark Connector (org.neo4j.spark.DataSource)\")\nprint(\"=\" * 100)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Sample Data Preview\n\nPreview sample records from each extracted node type to verify data quality and schema correctness."
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "metadata": {},
   "source": "# Display sample data from each node type for verification\nfor node_label in node_types:\n    stats = extraction_stats[node_label]\n    table_name = stats[\"table_name\"]\n    \n    print(f\"\\n{'=' * 100}\")\n    print(f\"Sample Data: {node_label} ({stats['record_count']} total records)\")\n    print(f\"Table: {table_name}\")\n    print(f\"{'=' * 100}\")\n    \n    if stats[\"record_count\"] > 0:\n        # Show schema\n        sample_df = spark.sql(f\"SELECT * FROM {table_name} LIMIT 3\")\n        print(f\"\\nSchema:\")\n        sample_df.printSchema()\n        \n        print(f\"\\nSample Records (3 of {stats['record_count']}):\")\n        display(sample_df)\n    else:\n        print(\"‚ö†Ô∏è  No records to display\")\n    \n    print()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Detailed Schema Inspection\n",
    "\n",
    "Examine the detailed schema and metadata for the Customer table to verify field types, constraints, and Delta table properties."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "# Show detailed schema for Customer table as an example\n",
    "example_table = NODE_TABLE_NAMES[\"Customer\"]\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(f\"Detailed Schema Inspection: Customer Table\")\n",
    "print(f\"Table: {example_table}\")\n",
    "print(f\"{'=' * 100}\\n\")\n",
    "\n",
    "# Show schema in tree format\n",
    "customer_df = spark.table(example_table)\n",
    "print(\"Schema Structure:\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"Extended Table Metadata:\")\n",
    "print(\"=\" * 100)\n",
    "display(spark.sql(f\"DESCRIBE TABLE EXTENDED {example_table}\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "w79o4v74sn",
   "source": "---\n\n## PART 2: Relationship (Edge) Extraction\n\nExtract relationship data from Neo4j to create graph edge tables in Delta Lake. This enables graph analytics, path queries, and network analysis using standard SQL and Spark.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "kiuba4ecj2p",
   "source": "## Define Relationship Extraction Function\n\nDefine the function that extracts relationships (edges) from Neo4j to Delta Lake using the **`relationship` option approach**.\n\nThis approach:\n- ‚úÖ Automatically includes relationship metadata (`<rel.id>`, `<rel.type>`, source/target IDs)\n- ‚úÖ Uses business-meaningful column names (e.g., `customerId`, `senderAccountId`)\n- ‚úÖ Simple, clean code following best practices from RELATIONSHIP_HANDLING.md\n- ‚úÖ Ready for graph analytics and path queries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e6s70zayped",
   "source": "def extract_relationship_type_to_delta(rel_type: str, limit: int = 100) -> Dict[str, Any]:\n    \"\"\"\n    Extract relationships from Neo4j using Spark Connector and write to Delta table.\n    \n    Args:\n        rel_type (str): Neo4j relationship type (e.g., 'HAS_ACCOUNT', 'PERFORMS')\n        limit (int): Maximum number of records to extract (default: 100)\n    \n    Returns:\n        Dict[str, Any]: Extraction statistics\n    \"\"\"\n    print(f\"\\n{'=' * 70}\")\n    print(f\"Extracting {rel_type} relationships...\")\n    print(f\"{'=' * 70}\")\n    \n    try:\n        # Get metadata for this relationship type\n        metadata = get_relationship_metadata(rel_type)\n        \n        source_label = metadata[\"source_label\"]\n        dest_label = metadata[\"destination_label\"]\n        source_key = metadata[\"source_key\"]  # Actual property name on source node\n        dest_key = metadata[\"destination_key\"]  # Actual property name on target node\n        \n        # Get schema to determine output column names\n        from neo4j_schemas import BASE_RELATIONSHIP_SCHEMAS\n        schema = BASE_RELATIONSHIP_SCHEMAS[rel_type]\n        output_source_col = schema.fields[0].name  # First field is source key\n        output_dest_col = schema.fields[1].name    # Second field is destination key\n        \n        print(f\"  ‚öôÔ∏è  Pattern: ({source_label})-[:{rel_type}]->({dest_label})\")\n        print(f\"  ‚öôÔ∏è  Neo4j Keys: {source_key} ‚Üí {dest_key}\")\n        print(f\"  ‚öôÔ∏è  Output Columns: {output_source_col} ‚Üí {output_dest_col}\")\n        \n        # Read from Neo4j using relationship option in FLAT MODE\n        df = (spark.read\n              .format(\"org.neo4j.spark.DataSource\")\n              .option(\"url\", neo4j_url)\n              .option(\"authentication.type\", \"basic\")\n              .option(\"authentication.basic.username\", neo4j_username)\n              .option(\"authentication.basic.password\", neo4j_password)\n              .option(\"database\", neo4j_database)\n              .option(\"relationship\", rel_type)\n              .option(\"relationship.source.labels\", source_label)\n              .option(\"relationship.target.labels\", dest_label)\n              .option(\"relationship.nodes.map\", \"false\")\n              .load())\n        \n        # Build column selection using actual Neo4j property names\n        source_col = f\"`source.{source_key}`\"\n        dest_col = f\"`target.{dest_key}`\"\n        \n        # Select and rename columns to match schema expectations\n        df_final = (df\n                    .select(\n                        col(source_col).alias(output_source_col),\n                        col(dest_col).alias(output_dest_col),\n                        col(\"`<rel.id>`\").alias(\"rel_element_id\"),\n                        col(\"`<rel.type>`\").alias(\"rel_type\"),\n                        col(\"`<source.id>`\").alias(\"src_neo4j_id\"),\n                        col(\"`<target.id>`\").alias(\"dst_neo4j_id\")\n                    )\n                    .withColumn(\"ingestion_timestamp\", current_timestamp())\n                    .limit(limit))\n        \n        # Get table name from schema module\n        table_name = RELATIONSHIP_TABLE_NAMES[rel_type]\n        \n        # Write to Delta table\n        print(f\"  ‚öôÔ∏è  Writing to Delta table...\")\n        (df_final.write\n         .format(\"delta\")\n         .mode(\"overwrite\")\n         .option(\"overwriteSchema\", \"true\")\n         .saveAsTable(table_name))\n        \n        # Get record count\n        count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").first()[\"count\"]\n        \n        print(f\"  ‚úÖ Complete: {count} edges ‚Üí {table_name}\")\n        \n        return {\n            \"rel_type\": rel_type,\n            \"record_count\": count,\n            \"table_name\": table_name,\n            \"pattern\": f\"{source_label} ‚Üí {dest_label}\",\n            \"status\": \"success\"\n        }\n        \n    except Exception as e:\n        print(f\"  ‚ùå ERROR: {str(e)}\")\n        return {\n            \"rel_type\": rel_type,\n            \"record_count\": 0,\n            \"table_name\": RELATIONSHIP_TABLE_NAMES.get(rel_type, \"unknown\"),\n            \"pattern\": \"unknown\",\n            \"status\": \"error\",\n            \"error\": str(e)\n        }\n\nprint(\"‚úÖ Relationship extraction function defined\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "q8vkiypaduf",
   "source": "## Extract All Relationship Types\n\nExtract all 7 relationship types from Neo4j to Delta Lake.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1ird8qmk1k1",
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"EXTRACTING RELATIONSHIP DATA FROM NEO4J TO DELTA TABLES\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Define relationship types to extract - aligned with neo4j_schemas.py definitions\nrelationship_types = [\n    \"HAS_ACCOUNT\",\n    \"AT_BANK\",\n    \"OF_COMPANY\",\n    \"PERFORMS\",\n    \"BENEFITS_TO\",\n    \"HAS_POSITION\",\n    \"OF_SECURITY\",\n]\n\n# Track extraction statistics\nrelationship_stats = {}\n\n# Extract each relationship type\nfor rel_type in relationship_types:\n    stats = extract_relationship_type_to_delta(rel_type, limit=100)\n    relationship_stats[rel_type] = stats\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ RELATIONSHIP EXTRACTION COMPLETE\")\nprint(\"=\" * 80)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1te12zlc5zy",
   "source": "## Relationship Extraction Summary\n\nDisplay statistics about the relationship extraction process.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "urj01yt0sz",
   "source": "# Create summary DataFrame with relationship extraction results\nrel_summary_data = []\nfor rel_type in relationship_types:\n    stats = relationship_stats[rel_type]\n    \n    # Determine status display\n    if stats[\"status\"] == \"error\":\n        status_display = \"‚ùå Error\"\n        error_msg = stats.get(\"error\", \"Unknown error\")\n    elif stats[\"record_count\"] == 0:\n        status_display = \"‚ö†Ô∏è  Empty\"\n        error_msg = \"No records found\"\n    else:\n        status_display = \"‚úÖ Success\"\n        error_msg = \"OK\"\n    \n    rel_summary_data.append({\n        \"Relationship\": rel_type,\n        \"Pattern\": stats[\"pattern\"],\n        \"Records\": stats[\"record_count\"],\n        \"Delta Table\": stats[\"table_name\"],\n        \"Status\": status_display,\n        \"Details\": error_msg if stats[\"status\"] != \"success\" else \"OK\"\n    })\n\nrel_summary_df = pd.DataFrame(rel_summary_data)\n\nprint(\"\\n\" + \"=\" * 100)\nprint(\"RELATIONSHIP EXTRACTION SUMMARY\")\nprint(\"=\" * 100)\nprint(rel_summary_df.to_string(index=False))\nprint(\"\\n\" + \"=\" * 100)\nprint(f\"Total Edge Tables Created: {len(relationship_stats)}\")\nprint(f\"Total Edges Extracted: {sum(s['record_count'] for s in relationship_stats.values())}\")\nprint(f\"Successful Extractions: {sum(1 for s in relationship_stats.values() if s['status'] == 'success')}\")\nprint(f\"Catalog Location: fintech.default\")\nprint(f\"Extraction Method: Neo4j Spark Connector (org.neo4j.spark.DataSource) with relationship option\")\nprint(\"=\" * 100)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "raimcynvolq",
   "source": "## Sample Relationship Queries\n\nDemonstrate how to use the relationship tables with joins to perform graph analytics using standard SQL.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "01ptjcurem3f",
   "source": "# Example 1: Customer Accounts with Bank Information\n# Join: Customer -[HAS_ACCOUNT]-> Account -[AT_BANK]-> Bank\n\nprint(\"=\" * 100)\nprint(\"Example 1: Customer Accounts with Bank Information\")\nprint(\"=\" * 100)\n\nquery1 = \"\"\"\nSELECT \n    c.customerId,\n    c.firstName,\n    c.lastName,\n    a.accountId,\n    a.accountType,\n    a.balance,\n    b.name AS bank_name,\n    b.bankType\nFROM fintech.default.neo4j_customer c\nJOIN fintech.default.neo4j_has_account ha ON c.customerId = ha.customerId\nJOIN fintech.default.neo4j_account a ON ha.accountId = a.accountId\nJOIN fintech.default.neo4j_at_bank ab ON a.accountId = ab.accountId\nJOIN fintech.default.neo4j_bank b ON ab.bankId = b.bankId\nORDER BY a.balance DESC\nLIMIT 10\n\"\"\"\n\nresult1 = spark.sql(query1)\nprint(\"\\nTop 10 Accounts by Balance with Customer and Bank Information:\")\ndisplay(result1)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "oh2macbfy0h",
   "source": [
    "# Example 2: Portfolio Holdings Analysis\n",
    "# Join: Account -[HAS_POSITION]-> Position -[OF_SECURITY]-> Stock -[OF_COMPANY]-> Company\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 3: Portfolio Holdings with Stock and Company Details\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    hp.accountId,\n",
    "    p.positionId,\n",
    "    p.shares,\n",
    "    p.currentValue,\n",
    "    p.percentageOfPortfolio,\n",
    "    s.ticker,\n",
    "    s.currentPrice,\n",
    "    co.name AS company_name,\n",
    "    co.sector,\n",
    "    co.industry\n",
    "FROM fintech.default.neo4j_has_position hp\n",
    "JOIN fintech.default.neo4j_position p ON hp.positionId = p.positionId\n",
    "JOIN fintech.default.neo4j_of_security os ON p.positionId = os.positionId\n",
    "JOIN fintech.default.neo4j_stock s ON os.stockId = s.stockId\n",
    "JOIN fintech.default.neo4j_of_company oc ON s.stockId = oc.stockId\n",
    "JOIN fintech.default.neo4j_company co ON oc.companyId = co.companyId\n",
    "ORDER BY p.currentValue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result3 = spark.sql(query3)\n",
    "print(\"\\nTop 10 Portfolio Positions by Value:\")\n",
    "display(result3)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "wstig6ed5ee",
   "source": [
    "# Example 3: Complete Customer Financial Profile\n",
    "# Multi-path join showing customer's accounts, transactions, and investments\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 4: Complete Financial Profile for a Sample Customer\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "query4 = \"\"\"\n",
    "WITH customer_accounts AS (\n",
    "    SELECT \n",
    "        c.customerId,\n",
    "        c.firstName,\n",
    "        c.lastName,\n",
    "        c.riskProfile,\n",
    "        a.accountId,\n",
    "        a.accountType,\n",
    "        a.balance\n",
    "    FROM fintech.default.neo4j_customer c\n",
    "    JOIN fintech.default.neo4j_has_account ha ON c.customerId = ha.customerId\n",
    "    JOIN fintech.default.neo4j_account a ON ha.accountId = a.accountId\n",
    "),\n",
    "account_positions AS (\n",
    "    SELECT \n",
    "        hp.accountId,\n",
    "        COUNT(*) AS num_positions,\n",
    "        SUM(p.currentValue) AS total_portfolio_value\n",
    "    FROM fintech.default.neo4j_has_position hp\n",
    "    JOIN fintech.default.neo4j_position p ON hp.positionId = p.positionId\n",
    "    GROUP BY hp.accountId\n",
    ")\n",
    "/*\n",
    ",account_transactions AS (\n",
    "    SELECT\n",
    "        p.senderAccountId AS accountId,\n",
    "        COUNT(*) AS num_transactions,\n",
    "        SUM(t.amount) AS total_sent\n",
    "    FROM fintech.default.neo4j_performs p\n",
    "    JOIN fintech.default.neo4j_transaction t ON p.transactionId = t.transactionId\n",
    "    GROUP BY p.senderAccountId\n",
    ")\n",
    "*/\n",
    "SELECT\n",
    "    ca.customerId,\n",
    "    ca.firstName,\n",
    "    ca.lastName,\n",
    "    ca.riskProfile,\n",
    "    ca.accountId,\n",
    "    ca.accountType,\n",
    "    ca.balance,\n",
    "    COALESCE(ap.num_positions, 0) AS num_positions,\n",
    "    COALESCE(ap.total_portfolio_value, 0) AS portfolio_value\n",
    "    -- ,COALESCE(at.num_transactions, 0) AS num_transactions_sent\n",
    "    -- ,COALESCE(at.total_sent, 0) AS total_amount_sent\n",
    "FROM customer_accounts ca\n",
    "LEFT JOIN account_positions ap ON ca.accountId = ap.accountId\n",
    "-- LEFT JOIN account_transactions at ON ca.accountId = at.accountId\n",
    "ORDER BY ca.balance DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result4 = spark.sql(query4)\n",
    "print(\"\\nComplete Financial Profile (Top 10 Customers by Account Balance):\")\n",
    "display(result4)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}