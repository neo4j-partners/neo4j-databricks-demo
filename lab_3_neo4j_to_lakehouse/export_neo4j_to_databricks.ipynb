{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Export Neo4j Graph Data to Databricks Unity Catalog\n",
    "\n",
    "This notebook extracts nodes and relationships from Neo4j and writes them as Delta tables in Unity Catalog.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Neo4j database** with financial demo data loaded\n",
    "2. **Databricks Secrets** - run `./scripts/setup_databricks_secrets.sh`\n",
    "3. **Databricks cluster** with Neo4j Spark Connector installed\n",
    "4. **Cluster access mode**: Dedicated (not Shared)\n",
    "\n",
    "## Output\n",
    "\n",
    "Creates 14 Delta tables in Unity Catalog:\n",
    "- 7 node tables: customer, bank, account, company, stock, position, transaction\n",
    "- 7 relationship tables: has_account, at_bank, of_company, performs, benefits_to, has_position, of_security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Define the node labels and relationship types to export from Neo4j. The catalog and schema names are set as defaults but will be overridden by values from Databricks Secrets in the next step."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Default catalog/schema (will be overridden from secrets)\n",
    "CATALOG = \"neo4j_augmentation_demo\"\n",
    "SCHEMA = \"graph_data\"\n",
    "\n",
    "# Node labels to extract\n",
    "NODE_LABELS = [\n",
    "    \"Customer\",\n",
    "    \"Bank\",\n",
    "    \"Account\",\n",
    "    \"Company\",\n",
    "    \"Stock\",\n",
    "    \"Position\",\n",
    "    \"Transaction\",\n",
    "]\n",
    "\n",
    "# Relationships: (type, source_label, target_label)\n",
    "RELATIONSHIPS = [\n",
    "    (\"HAS_ACCOUNT\", \"Customer\", \"Account\"),\n",
    "    (\"AT_BANK\", \"Account\", \"Bank\"),\n",
    "    (\"OF_COMPANY\", \"Stock\", \"Company\"),\n",
    "    (\"PERFORMS\", \"Account\", \"Transaction\"),\n",
    "    (\"BENEFITS_TO\", \"Transaction\", \"Account\"),\n",
    "    (\"HAS_POSITION\", \"Account\", \"Position\"),\n",
    "    (\"OF_SECURITY\", \"Position\", \"Stock\"),\n",
    "]\n",
    "\n",
    "print(f\"Nodes to export: {len(NODE_LABELS)}\")\n",
    "print(f\"Relationships to export: {len(RELATIONSHIPS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Load Credentials\n",
    "\n",
    "Retrieve Neo4j connection credentials and Unity Catalog configuration from Databricks Secrets. The volume path secret is parsed to extract the catalog and schema names, ensuring consistency with other labs."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading credentials from Databricks Secrets...\")\n",
    "\n",
    "NEO4J_URL = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"url\")\n",
    "NEO4J_USER = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"username\")\n",
    "NEO4J_PASS = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"password\")\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Extract catalog/schema from volume_path: /Volumes/{catalog}/{schema}/{volume}\n",
    "try:\n",
    "    volume_path = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"volume_path\")\n",
    "    parts = volume_path.strip(\"/\").split(\"/\")\n",
    "    if len(parts) >= 3 and parts[0] == \"Volumes\":\n",
    "        CATALOG = parts[1]\n",
    "        SCHEMA = parts[2]\n",
    "        print(f\"[OK] Catalog: {CATALOG}\")\n",
    "        print(f\"[OK] Schema: {SCHEMA}\")\n",
    "except Exception:\n",
    "    print(f\"[INFO] Using defaults: {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Configure Spark for Neo4j\n",
    "spark.conf.set(\"neo4j.url\", NEO4J_URL)\n",
    "spark.conf.set(\"neo4j.authentication.basic.username\", NEO4J_USER)\n",
    "spark.conf.set(\"neo4j.authentication.basic.password\", NEO4J_PASS)\n",
    "spark.conf.set(\"neo4j.database\", NEO4J_DATABASE)\n",
    "\n",
    "print(f\"[OK] Neo4j URL: {NEO4J_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Test Neo4j Connection\n",
    "\n",
    "Verify that the Spark session can connect to Neo4j using the configured credentials. This runs a simple Cypher query to confirm the Neo4j Spark Connector is working before attempting the full export."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Neo4j connection...\")\n",
    "\n",
    "try:\n",
    "    test_df = spark.read.format(\"org.neo4j.spark.DataSource\").option(\"query\", \"RETURN 1 AS test\").load()\n",
    "    test_df.collect()\n",
    "    print(\"[OK] Connected to Neo4j!\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Setup Unity Catalog\n",
    "\n",
    "Verify the target catalog exists and create the schema if it doesn't exist. The schema will hold all the Delta tables created during the export process."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up Unity Catalog...\")\n",
    "\n",
    "# Check catalog exists\n",
    "try:\n",
    "    catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if CATALOG not in catalogs:\n",
    "        print(f\"[ERROR] Catalog '{CATALOG}' not found.\")\n",
    "        print(f\"Available: {catalogs}\")\n",
    "        raise ValueError(f\"Catalog '{CATALOG}' not found\")\n",
    "    print(f\"[OK] Catalog exists: {CATALOG}\")\n",
    "except ValueError:\n",
    "    raise\n",
    "except Exception:\n",
    "    print(f\"[INFO] Could not list catalogs, trying to use '{CATALOG}' directly\")\n",
    "\n",
    "# Create schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "print(f\"[OK] Schema ready: {CATALOG}.{SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Define Helper Functions\n",
    "\n",
    "Create reusable functions for reading data from Neo4j and writing to Delta tables:\n",
    "- `read_nodes()`: Reads all nodes with a given label from Neo4j\n",
    "- `read_relationship()`: Reads relationships of a given type between specified node labels\n",
    "- `write_table()`: Writes a DataFrame to a Delta table in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nodes(label):\n",
    "    \"\"\"Read nodes from Neo4j.\"\"\"\n",
    "    return spark.read.format(\"org.neo4j.spark.DataSource\").option(\"labels\", label).load()\n",
    "\n",
    "def read_relationship(rel_type, source_label, target_label):\n",
    "    \"\"\"Read relationships from Neo4j.\"\"\"\n",
    "    return (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"relationship\", rel_type)\n",
    "        .option(\"relationship.source.labels\", source_label)\n",
    "        .option(\"relationship.target.labels\", target_label)\n",
    "        .option(\"relationship.nodes.map\", \"false\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "def write_table(df, table_name):\n",
    "    \"\"\"Write DataFrame to Delta table.\"\"\"\n",
    "    full_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    return df.count()\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Export Nodes\n",
    "\n",
    "Iterate through all node labels defined in the configuration, read each node type from Neo4j, and write them as Delta tables. Progress and timing information is displayed for each table."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING NODES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "node_results = {}\n",
    "\n",
    "for i, label in enumerate(NODE_LABELS, 1):\n",
    "    table_name = label.lower()\n",
    "    print(f\"\\n[{i}/{len(NODE_LABELS)}] {label} -> {table_name}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    df = read_nodes(label)\n",
    "    count = write_table(df, table_name)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    node_results[label] = {\"count\": count, \"time\": elapsed}\n",
    "    print(f\"    [OK] {count} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nExported {len(node_results)} node tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Export Relationships\n",
    "\n",
    "Iterate through all relationship types defined in the configuration, read each relationship from Neo4j (including source and target node IDs), and write them as Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING RELATIONSHIPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rel_results = {}\n",
    "\n",
    "for i, (rel_type, source, target) in enumerate(RELATIONSHIPS, 1):\n",
    "    table_name = rel_type.lower()\n",
    "    print(f\"\\n[{i}/{len(RELATIONSHIPS)}] {rel_type} -> {table_name}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    df = read_relationship(rel_type, source, target)\n",
    "    count = write_table(df, table_name)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    rel_results[rel_type] = {\"count\": count, \"time\": elapsed}\n",
    "    print(f\"    [OK] {count} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nExported {len(rel_results)} relationship tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 8: Validate\n",
    "\n",
    "Compare the exported row counts against expected values to ensure the export completed correctly. Any mismatches indicate potential issues with the source data or export process."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-16",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "expected_nodes = {\n",
    "    \"Customer\": 102, \"Bank\": 102, \"Account\": 123, \"Company\": 102,\n",
    "    \"Stock\": 102, \"Position\": 110, \"Transaction\": 123,\n",
    "}\n",
    "\n",
    "expected_rels = {\n",
    "    \"HAS_ACCOUNT\": 123, \"AT_BANK\": 123, \"OF_COMPANY\": 102, \"PERFORMS\": 123,\n",
    "    \"BENEFITS_TO\": 123, \"HAS_POSITION\": 110, \"OF_SECURITY\": 110,\n",
    "}\n",
    "\n",
    "all_valid = True\n",
    "\n",
    "print(f\"\\n{'Table':<15} {'Expected':>10} {'Actual':>10} {'Status':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label, expected in expected_nodes.items():\n",
    "    actual = node_results.get(label, {}).get(\"count\", 0)\n",
    "    status = \"OK\" if actual == expected else \"MISMATCH\"\n",
    "    if actual != expected: all_valid = False\n",
    "    print(f\"{label.lower():<15} {expected:>10} {actual:>10} {status:>10}\")\n",
    "\n",
    "for rel_type, expected in expected_rels.items():\n",
    "    actual = rel_results.get(rel_type, {}).get(\"count\", 0)\n",
    "    status = \"OK\" if actual == expected else \"MISMATCH\"\n",
    "    if actual != expected: all_valid = False\n",
    "    print(f\"{rel_type.lower():<15} {expected:>10} {actual:>10} {status:>10}\")\n",
    "\n",
    "print(f\"\\n{'All validations passed!' if all_valid else 'Some counts do not match'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 9: Summary\n",
    "\n",
    "Display a final summary of the export operation, including the destination catalog/schema, total number of tables created, and total row count across all tables."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-18",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = sum(r[\"count\"] for r in node_results.values())\n",
    "total_rels = sum(r[\"count\"] for r in rel_results.values())\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Destination: {CATALOG}.{SCHEMA}\")\n",
    "print(f\"Tables: {len(node_results) + len(rel_results)}\")\n",
    "print(f\"Total rows: {total_nodes + total_rels}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
