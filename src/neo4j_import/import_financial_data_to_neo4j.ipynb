{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Demo Data Import to Neo4j\n",
    "\n",
    "This notebook imports the retail banking and investment portfolio demonstration data from Databricks Unity Catalog into Neo4j.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Neo4j database** running (Aura or self-hosted)\n",
    "2. **Databricks Secrets** configured with `neo4j-creds` scope containing `username`, `password`, and `url`\n",
    "3. **Unity Catalog Volume** with CSV files uploaded\n",
    "4. **Databricks Cluster** with Neo4j Spark Connector installed\n",
    "\n",
    "## Data Overview\n",
    "\n",
    "- **102 Customers** with demographics and financial profiles\n",
    "- **102 Banks** across multiple types (commercial, regional, community)\n",
    "- **123 Accounts** (checking, savings, investment)\n",
    "- **102 Companies** across 12+ sectors\n",
    "- **102 Stocks** with market data\n",
    "- **110 Portfolio Positions** linking accounts to stocks\n",
    "- **123 Transactions** between accounts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Configure Neo4j connection and data source paths. All credentials are retrieved from Databricks Secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION - Loading secrets from Databricks\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\")\n",
    "\n",
    "# Retrieve Neo4j credentials from Databricks Secrets\n",
    "print(\"[DEBUG] Retrieving secrets from scope 'neo4j-creds'...\")\n",
    "\n",
    "try:\n",
    "    NEO4J_USER = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"username\")\n",
    "    print(f\"  [OK] username: retrieved ({len(NEO4J_USER)} chars)\")\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] username: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    NEO4J_PASS = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"password\")\n",
    "    print(f\"  [OK] password: retrieved ({len(NEO4J_PASS)} chars, masked)\")\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] password: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    NEO4J_URL = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"url\")\n",
    "    print(f\"  [OK] url: {NEO4J_URL}\")\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] url: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    VOLUME_PATH = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"volume_path\")\n",
    "    print(f\"  [OK] volume_path: {VOLUME_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] volume_path: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Neo4j database name (default: neo4j)\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "print(\"\")\n",
    "print(\"[DEBUG] Configuring Spark session for Neo4j connector...\")\n",
    "try:\n",
    "    spark.conf.set(\"neo4j.url\", NEO4J_URL)\n",
    "    spark.conf.set(\"neo4j.authentication.basic.username\", NEO4J_USER)\n",
    "    spark.conf.set(\"neo4j.authentication.basic.password\", NEO4J_PASS)\n",
    "    spark.conf.set(\"neo4j.database\", NEO4J_DATABASE)\n",
    "    print(\"  [OK] Spark session configured\")\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] Spark configuration: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Neo4j URL:    {NEO4J_URL}\")\n",
    "print(f\"  Database:     {NEO4J_DATABASE}\")\n",
    "print(f\"  Username:     {NEO4J_USER}\")\n",
    "print(f\"  Volume Path:  {VOLUME_PATH}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Prerequisites\n",
    "\n",
    "Verify Neo4j connectivity and CSV file availability before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFY CSV FILES IN UNITY CATALOG VOLUME\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"FILE VERIFICATION - Checking CSV files in Unity Catalog Volume\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# CSV files are in the /csv subdirectory of the volume\n",
    "CSV_PATH = f\"{VOLUME_PATH}/csv\"\n",
    "print(f\"CSV path: {CSV_PATH}\")\n",
    "print(\"\")\n",
    "\n",
    "expected_files = [\n",
    "    \"customers.csv\",\n",
    "    \"banks.csv\",\n",
    "    \"accounts.csv\",\n",
    "    \"companies.csv\",\n",
    "    \"stocks.csv\",\n",
    "    \"portfolio_holdings.csv\",\n",
    "    \"transactions.csv\"\n",
    "]\n",
    "\n",
    "print(f\"[DEBUG] Expected files: {len(expected_files)}\")\n",
    "for f in expected_files:\n",
    "    print(f\"  - {f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Try listing files\n",
    "print(f\"[DEBUG] Calling dbutils.fs.ls('{CSV_PATH}')...\")\n",
    "try:\n",
    "    files = dbutils.fs.ls(CSV_PATH)\n",
    "    print(f\"  [OK] Listed {len(files)} items\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Show raw file info for debugging\n",
    "    print(\"[DEBUG] Raw file listing from dbutils.fs.ls():\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, f in enumerate(files):\n",
    "        print(f\"  [{i}] name: {f.name}, size: {f.size} bytes\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Extract filenames - handle various formats\n",
    "    found_files = []\n",
    "    for f in files:\n",
    "        name = f.name.rstrip('/')\n",
    "        if '/' in name:\n",
    "            name = name.split('/')[-1]\n",
    "        found_files.append(name)\n",
    "    \n",
    "    # Check for expected files\n",
    "    print(\"[DEBUG] Checking for expected files:\")\n",
    "    print(\"-\" * 70)\n",
    "    all_present = True\n",
    "    for expected in expected_files:\n",
    "        found = expected in found_files\n",
    "        status = \"[OK]  \" if found else \"[MISSING]\"\n",
    "        if not found:\n",
    "            all_present = False\n",
    "        print(f\"  {status} {expected}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if all_present:\n",
    "        print(\"\\n[OK] All required CSV files are present!\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Some files are missing!\")\n",
    "        print(\"Found files:\", found_files)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] Error listing files: {type(e).__name__}: {str(e)}\")\n",
    "    print(\"\")\n",
    "    print(\"[WARNING] Could not list files. Will attempt to read files directly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFY NEO4J CONNECTIVITY\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"NEO4J CONNECTION TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"URL: {NEO4J_URL}\")\n",
    "print(f\"Database: {NEO4J_DATABASE}\")\n",
    "print(f\"Username: {NEO4J_USER}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"[DEBUG] Building Spark DataSource read...\")\n",
    "print(\"  Format: org.neo4j.spark.DataSource\")\n",
    "print(f\"  Query: RETURN 'Connected' AS status\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    print(\"[DEBUG] Executing connection test...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_df = (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", NEO4J_URL)\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "        .option(\"database\", NEO4J_DATABASE)\n",
    "        .option(\"query\", \"RETURN 'Connected' AS status\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    result = test_df.collect()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"  [OK] Query executed in {elapsed:.2f}s\")\n",
    "    print(f\"  [OK] Result: {result}\")\n",
    "    print(\"\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"[OK] NEO4J CONNECTION SUCCESSFUL!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] Connection failed!\")\n",
    "    print(f\"  Error type: {type(e).__name__}\")\n",
    "    print(f\"  Error message: {str(e)}\")\n",
    "    print(\"\")\n",
    "    print(\"[DEBUG] Troubleshooting tips:\")\n",
    "    print(\"  1. Verify Neo4j database is running\")\n",
    "    print(\"  2. Check URL format:\")\n",
    "    print(\"     - Aura: neo4j+s://xxxxx.databases.neo4j.io\")\n",
    "    print(\"     - Self-hosted with TLS: neo4j+s://host:7687\")\n",
    "    print(\"     - Self-hosted no TLS: bolt://host:7687\")\n",
    "    print(\"  3. Verify credentials are correct\")\n",
    "    print(\"  4. Check network connectivity (firewall, VPC)\")\n",
    "    print(\"  5. Ensure Neo4j Spark Connector is installed on cluster\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CLEAR EXISTING DATABASE\n",
    "# =============================================================================\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATABASE CLEANUP - Removing existing nodes and relationships\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "print(\"[DEBUG] Deleting all nodes and relationships...\")\n",
    "print(\"        Query: MATCH (n) DETACH DELETE n\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use neo4j Python driver for direct Cypher execution\n",
    "    driver = GraphDatabase.driver(\n",
    "        NEO4J_URL,\n",
    "        auth=(NEO4J_USER, NEO4J_PASS)\n",
    "    )\n",
    "\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        result = session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        summary = result.consume()\n",
    "        nodes_deleted = summary.counters.nodes_deleted\n",
    "        rels_deleted = summary.counters.relationships_deleted\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  [OK] Deleted {nodes_deleted} nodes and {rels_deleted} relationships\")\n",
    "    print(f\"  [OK] Completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"[OK] DATABASE CLEANUP COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] Cleanup failed: {type(e).__name__}\")\n",
    "    print(f\"         {str(e)[:200]}\")\n",
    "    raise"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Helper Functions\n",
    "\n",
    "Define reusable functions for data loading and Neo4j operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, DateType\n",
    "\n",
    "\n",
    "def read_csv(filename: str) -> DataFrame:\n",
    "    \"\"\"Read a CSV file from the Unity Catalog Volume /csv subdirectory.\"\"\"\n",
    "    path = f\"{CSV_PATH}/{filename}\"\n",
    "    return spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "\n",
    "def write_nodes(df: DataFrame, label: str, node_key: str) -> dict:\n",
    "    \"\"\"Write DataFrame rows as nodes to Neo4j with debug logging.\"\"\"\n",
    "    count = df.count()\n",
    "    print(f\"[DEBUG] write_nodes(label={label}, key={node_key}, count={count})\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        (\n",
    "            df.write.format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"labels\", f\":{label}\")\n",
    "            .option(\"node.keys\", node_key)\n",
    "            .save()\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  [OK] {count} {label} nodes written in {elapsed:.2f}s\")\n",
    "        return {\"status\": \"OK\", \"count\": count, \"elapsed\": elapsed}\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] Error writing {label} nodes: {type(e).__name__}\")\n",
    "        print(f\"         {str(e)[:200]}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def write_relationship(\n",
    "    df: DataFrame,\n",
    "    rel_type: str,\n",
    "    source_label: str,\n",
    "    source_key: str,\n",
    "    target_label: str,\n",
    "    target_key: str\n",
    ") -> dict:\n",
    "    \"\"\"Write DataFrame rows as relationships to Neo4j with debug logging.\"\"\"\n",
    "    count = df.count()\n",
    "    print(f\"[DEBUG] write_relationship(type={rel_type})\")\n",
    "    print(f\"        Pattern: (:{source_label})-[:{rel_type}]->(:{target_label})\")\n",
    "    print(f\"        Keys: {source_key} -> {target_key}, count={count}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        (\n",
    "            df.write.format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"relationship\", rel_type)\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.source.labels\", f\":{source_label}\")\n",
    "            .option(\"relationship.source.node.keys\", f\"{source_key}:{source_key}\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.labels\", f\":{target_label}\")\n",
    "            .option(\"relationship.target.node.keys\", f\"{target_key}:{target_key}\")\n",
    "            .save()\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  [OK] {count} {rel_type} relationships written in {elapsed:.2f}s\")\n",
    "        return {\"status\": \"OK\", \"count\": count, \"elapsed\": elapsed}\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] Error writing {rel_type} relationships: {type(e).__name__}\")\n",
    "        print(f\"         {str(e)[:200]}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def run_cypher(query: str, debug: bool = False) -> DataFrame:\n",
    "    \"\"\"Execute a Cypher query and return results as DataFrame.\"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] run_cypher: {query[:100]}...\")\n",
    "    return (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", NEO4J_URL)\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "        .option(\"database\", NEO4J_DATABASE)\n",
    "        .option(\"query\", query)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HELPER FUNCTIONS LOADED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Functions available:\")\n",
    "print(\"  - read_csv(filename) -> DataFrame\")\n",
    "print(\"  - write_nodes(df, label, node_key) -> dict\")\n",
    "print(\"  - write_relationship(df, rel_type, src_label, src_key, tgt_label, tgt_key) -> dict\")\n",
    "print(\"  - run_cypher(query, debug=False) -> DataFrame\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Transform CSV Data\n",
    "\n",
    "Load all CSV files and apply appropriate data type conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD CSV FILES FROM UNITY CATALOG VOLUME\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA LOADING - Reading CSV files from Unity Catalog Volume\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"CSV path: {CSV_PATH}\")\n",
    "print(\"\")\n",
    "\n",
    "def load_csv_with_debug(filename: str) -> \"DataFrame\":\n",
    "    \"\"\"Load CSV with detailed debug logging.\"\"\"\n",
    "    path = f\"{CSV_PATH}/{filename}\"\n",
    "    print(f\"[DEBUG] Loading: {filename}\")\n",
    "    print(f\"        Path: {path}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        df = spark.read.option(\"header\", \"true\").csv(path)\n",
    "        count = df.count()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  [OK] Loaded {count} rows in {elapsed:.2f}s\")\n",
    "        print(f\"       Columns: {df.columns}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] Error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "# Load all CSVs with debug output\n",
    "print(\"-\" * 70)\n",
    "customers_raw = load_csv_with_debug(\"customers.csv\")\n",
    "print(\"\")\n",
    "\n",
    "banks_raw = load_csv_with_debug(\"banks.csv\")\n",
    "print(\"\")\n",
    "\n",
    "accounts_raw = load_csv_with_debug(\"accounts.csv\")\n",
    "print(\"\")\n",
    "\n",
    "companies_raw = load_csv_with_debug(\"companies.csv\")\n",
    "print(\"\")\n",
    "\n",
    "stocks_raw = load_csv_with_debug(\"stocks.csv\")\n",
    "print(\"\")\n",
    "\n",
    "positions_raw = load_csv_with_debug(\"portfolio_holdings.csv\")\n",
    "print(\"\")\n",
    "\n",
    "transactions_raw = load_csv_with_debug(\"transactions.csv\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"[OK] ALL CSV FILES LOADED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "print(\"Summary:\")\n",
    "print(f\"  customers.csv:          {customers_raw.count()} rows\")\n",
    "print(f\"  banks.csv:              {banks_raw.count()} rows\")\n",
    "print(f\"  accounts.csv:           {accounts_raw.count()} rows\")\n",
    "print(f\"  companies.csv:          {companies_raw.count()} rows\")\n",
    "print(f\"  stocks.csv:             {stocks_raw.count()} rows\")\n",
    "print(f\"  portfolio_holdings.csv: {positions_raw.count()} rows\")\n",
    "print(f\"  transactions.csv:       {transactions_raw.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA TYPE CONVERSIONS\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA TRANSFORMATION - Applying data type conversions\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "def transform_with_debug(df: DataFrame, name: str, transformations: list) -> DataFrame:\n",
    "    \"\"\"Apply transformations with debug logging.\"\"\"\n",
    "    print(f\"[DEBUG] Transforming: {name}\")\n",
    "    result = df\n",
    "    for col_name, transform_type in transformations:\n",
    "        try:\n",
    "            if transform_type == \"INT\":\n",
    "                result = result.withColumn(col_name, F.col(col_name).cast(IntegerType()))\n",
    "            elif transform_type == \"DOUBLE\":\n",
    "                result = result.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "            elif transform_type == \"DATE\":\n",
    "                result = result.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "            print(f\"  [OK] {col_name} -> {transform_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FAIL] {col_name}: {str(e)[:50]}\")\n",
    "    return result\n",
    "\n",
    "# Transform Customers\n",
    "customers = transform_with_debug(customers_raw, \"Customers\", [\n",
    "    (\"annual_income\", \"INT\"),\n",
    "    (\"credit_score\", \"INT\"),\n",
    "    (\"registration_date\", \"DATE\"),\n",
    "    (\"date_of_birth\", \"DATE\"),\n",
    "])\n",
    "print(\"\")\n",
    "\n",
    "# Transform Banks\n",
    "banks = transform_with_debug(banks_raw, \"Banks\", [\n",
    "    (\"total_assets_billions\", \"DOUBLE\"),\n",
    "    (\"established_year\", \"INT\"),\n",
    "])\n",
    "print(\"\")\n",
    "\n",
    "# Transform Accounts\n",
    "accounts = transform_with_debug(accounts_raw, \"Accounts\", [\n",
    "    (\"balance\", \"DOUBLE\"),\n",
    "    (\"interest_rate\", \"DOUBLE\"),\n",
    "    (\"opened_date\", \"DATE\"),\n",
    "])\n",
    "print(\"\")\n",
    "\n",
    "# Transform Companies\n",
    "companies = transform_with_debug(companies_raw, \"Companies\", [\n",
    "    (\"market_cap_billions\", \"DOUBLE\"),\n",
    "    (\"annual_revenue_billions\", \"DOUBLE\"),\n",
    "    (\"founded_year\", \"INT\"),\n",
    "    (\"employee_count\", \"INT\"),\n",
    "])\n",
    "print(\"\")\n",
    "\n",
    "# Transform Stocks\n",
    "stocks = transform_with_debug(stocks_raw, \"Stocks\", [\n",
    "    (\"current_price\", \"DOUBLE\"),\n",
    "    (\"previous_close\", \"DOUBLE\"),\n",
    "    (\"opening_price\", \"DOUBLE\"),\n",
    "    (\"day_high\", \"DOUBLE\"),\n",
    "    (\"day_low\", \"DOUBLE\"),\n",
    "    (\"volume\", \"INT\"),\n",
    "    (\"market_cap_billions\", \"DOUBLE\"),\n",
    "    (\"pe_ratio\", \"DOUBLE\"),\n",
    "    (\"dividend_yield\", \"DOUBLE\"),\n",
    "    (\"fifty_two_week_high\", \"DOUBLE\"),\n",
    "    (\"fifty_two_week_low\", \"DOUBLE\"),\n",
    "])\n",
    "print(\"\")\n",
    "\n",
    "# Transform Positions (rename holding_id to position_id)\n",
    "print(\"[DEBUG] Transforming: Positions\")\n",
    "print(\"  [OK] holding_id -> position_id (renamed)\")\n",
    "positions = positions_raw.withColumnRenamed(\"holding_id\", \"position_id\")\n",
    "positions = transform_with_debug(positions, \"Positions (continued)\", [\n",
    "    (\"shares\", \"INT\"),\n",
    "    (\"purchase_price\", \"DOUBLE\"),\n",
    "    (\"current_value\", \"DOUBLE\"),\n",
    "    (\"percentage_of_portfolio\", \"DOUBLE\"),\n",
    "    (\"purchase_date\", \"DATE\"),\n",
    "])\n",
    "print(\"\")\n",
    "\n",
    "# Transform Transactions\n",
    "transactions = transform_with_debug(transactions_raw, \"Transactions\", [\n",
    "    (\"amount\", \"DOUBLE\"),\n",
    "    (\"transaction_date\", \"DATE\"),\n",
    "])\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"[OK] ALL DATA TRANSFORMATIONS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Indexes and Constraints\n",
    "\n",
    "Create indexes and uniqueness constraints BEFORE loading data for optimal performance.\n",
    "\n",
    "**Best Practice**: Creating indexes first significantly improves write performance for large datasets and ensures data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE INDEXES AND CONSTRAINTS\n",
    "# =============================================================================\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCHEMA SETUP - Creating indexes and constraints in Neo4j\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "print(\"[DEBUG] Creating uniqueness constraints...\")\n",
    "print(\"        (If constraint already exists, it will be skipped)\")\n",
    "print(\"\")\n",
    "\n",
    "# Define constraints\n",
    "constraints = [\n",
    "    (\"customer_id_unique\", \"Customer\", \"customer_id\"),\n",
    "    (\"bank_id_unique\", \"Bank\", \"bank_id\"),\n",
    "    (\"account_id_unique\", \"Account\", \"account_id\"),\n",
    "    (\"company_id_unique\", \"Company\", \"company_id\"),\n",
    "    (\"stock_id_unique\", \"Stock\", \"stock_id\"),\n",
    "    (\"position_id_unique\", \"Position\", \"position_id\"),\n",
    "    (\"transaction_id_unique\", \"Transaction\", \"transaction_id\"),\n",
    "]\n",
    "\n",
    "# Use neo4j Python driver for DDL operations\n",
    "driver = GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "success_count = 0\n",
    "skip_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "with driver.session(database=NEO4J_DATABASE) as session:\n",
    "    for constraint_name, label, property_name in constraints:\n",
    "        query = f\"\"\"\n",
    "        CREATE CONSTRAINT {constraint_name} IF NOT EXISTS\n",
    "        FOR (n:{label})\n",
    "        REQUIRE n.{property_name} IS UNIQUE\n",
    "        \"\"\"\n",
    "        print(f\"[DEBUG] Creating: {constraint_name}\")\n",
    "        print(f\"        FOR (n:{label}) REQUIRE n.{property_name} IS UNIQUE\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            session.run(query)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  [OK] Created/verified in {elapsed:.2f}s\")\n",
    "            success_count += 1\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"already exists\" in error_msg.lower():\n",
    "                print(f\"  [SKIP] Already exists\")\n",
    "                skip_count += 1\n",
    "            else:\n",
    "                print(f\"  [FAIL] {type(e).__name__}: {error_msg[:100]}\")\n",
    "                fail_count += 1\n",
    "        print(\"\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCHEMA SETUP SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Created/Verified: {success_count}\")\n",
    "print(f\"  Skipped (exists): {skip_count}\")\n",
    "print(f\"  Failed:           {fail_count}\")\n",
    "\n",
    "if fail_count > 0:\n",
    "    print(\"\\n[WARNING] Some constraints failed to create!\")\n",
    "else:\n",
    "    print(\"\\n[OK] All constraints ready!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Write Nodes to Neo4j\n",
    "\n",
    "Write all node types to Neo4j. The order doesn't matter since we're creating nodes first, then relationships.\n",
    "\n",
    "**Graph Schema:**\n",
    "- Customer (102 nodes)\n",
    "- Bank (102 nodes)\n",
    "- Account (123 nodes)\n",
    "- Company (102 nodes)\n",
    "- Stock (102 nodes)\n",
    "- Position (110 nodes)\n",
    "- Transaction (123 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WRITE NODES TO NEO4J\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"NODE CREATION - Writing nodes to Neo4j\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "node_results = {}\n",
    "total_start = time.time()\n",
    "\n",
    "# Customer nodes\n",
    "print(\"[1/7] CUSTOMER NODES\")\n",
    "print(\"-\" * 40)\n",
    "node_results[\"Customer\"] = write_nodes(customers, \"Customer\", \"customer_id\")\n",
    "print(\"\")\n",
    "\n",
    "# Bank nodes\n",
    "print(\"[2/7] BANK NODES\")\n",
    "print(\"-\" * 40)\n",
    "node_results[\"Bank\"] = write_nodes(banks, \"Bank\", \"bank_id\")\n",
    "print(\"\")\n",
    "\n",
    "# Account nodes (exclude foreign keys)\n",
    "print(\"[3/7] ACCOUNT NODES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"[DEBUG] Selecting columns (excluding foreign keys)...\")\n",
    "account_props = accounts.select(\n",
    "    \"account_id\", \"account_number\", \"account_type\", \n",
    "    \"balance\", \"currency\", \"opened_date\", \"status\", \"interest_rate\"\n",
    ")\n",
    "print(f\"        Columns: {account_props.columns}\")\n",
    "node_results[\"Account\"] = write_nodes(account_props, \"Account\", \"account_id\")\n",
    "print(\"\")\n",
    "\n",
    "# Company nodes\n",
    "print(\"[4/7] COMPANY NODES\")\n",
    "print(\"-\" * 40)\n",
    "node_results[\"Company\"] = write_nodes(companies, \"Company\", \"company_id\")\n",
    "print(\"\")\n",
    "\n",
    "# Stock nodes (exclude foreign key)\n",
    "print(\"[5/7] STOCK NODES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"[DEBUG] Selecting columns (excluding foreign keys)...\")\n",
    "stock_props = stocks.select(\n",
    "    \"stock_id\", \"ticker\", \"current_price\", \"previous_close\", \"opening_price\",\n",
    "    \"day_high\", \"day_low\", \"volume\", \"market_cap_billions\", \"pe_ratio\",\n",
    "    \"dividend_yield\", \"fifty_two_week_high\", \"fifty_two_week_low\", \"exchange\"\n",
    ")\n",
    "print(f\"        Columns: {stock_props.columns}\")\n",
    "node_results[\"Stock\"] = write_nodes(stock_props, \"Stock\", \"stock_id\")\n",
    "print(\"\")\n",
    "\n",
    "# Position nodes (exclude foreign keys)\n",
    "print(\"[6/7] POSITION NODES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"[DEBUG] Selecting columns (excluding foreign keys)...\")\n",
    "position_props = positions.select(\n",
    "    \"position_id\", \"shares\", \"purchase_price\", \"purchase_date\",\n",
    "    \"current_value\", \"percentage_of_portfolio\"\n",
    ")\n",
    "print(f\"        Columns: {position_props.columns}\")\n",
    "node_results[\"Position\"] = write_nodes(position_props, \"Position\", \"position_id\")\n",
    "print(\"\")\n",
    "\n",
    "# Transaction nodes (exclude foreign keys)\n",
    "print(\"[7/7] TRANSACTION NODES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"[DEBUG] Selecting columns (excluding foreign keys)...\")\n",
    "transaction_props = transactions.select(\n",
    "    \"transaction_id\", \"amount\", \"currency\", \"transaction_date\",\n",
    "    \"transaction_time\", \"type\", \"status\", \"description\"\n",
    ")\n",
    "print(f\"        Columns: {transaction_props.columns}\")\n",
    "node_results[\"Transaction\"] = write_nodes(transaction_props, \"Transaction\", \"transaction_id\")\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"NODE CREATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "print(f\"{'Label':<15} {'Count':>10} {'Time':>10} {'Status':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for label, result in node_results.items():\n",
    "    print(f\"{label:<15} {result['count']:>10} {result['elapsed']:>9.2f}s {'[OK]':>10}\")\n",
    "print(\"-\" * 50)\n",
    "total_nodes = sum(r['count'] for r in node_results.values())\n",
    "print(f\"{'TOTAL':<15} {total_nodes:>10} {total_elapsed:>9.2f}s\")\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"[OK] ALL NODES WRITTEN SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Write Relationships to Neo4j\n",
    "\n",
    "Create all relationships between nodes using key matching.\n",
    "\n",
    "**Relationship Types:**\n",
    "1. `(:Customer)-[:HAS_ACCOUNT]->(:Account)` - Customer owns account\n",
    "2. `(:Account)-[:AT_BANK]->(:Bank)` - Account held at bank\n",
    "3. `(:Stock)-[:OF_COMPANY]->(:Company)` - Stock issued by company\n",
    "4. `(:Account)-[:PERFORMS]->(:Transaction)` - Account initiates transfer\n",
    "5. `(:Transaction)-[:BENEFITS_TO]->(:Account)` - Account receives funds\n",
    "6. `(:Account)-[:HAS_POSITION]->(:Position)` - Account holds position\n",
    "7. `(:Position)-[:OF_SECURITY]->(:Stock)` - Position is in specific stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WRITE RELATIONSHIPS TO NEO4J\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"RELATIONSHIP CREATION - Writing relationships to Neo4j\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "rel_results = {}\n",
    "total_start = time.time()\n",
    "\n",
    "# 1. HAS_ACCOUNT: Customer -> Account\n",
    "print(\"[1/7] HAS_ACCOUNT RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "has_account_df = accounts.select(\"customer_id\", \"account_id\")\n",
    "rel_results[\"HAS_ACCOUNT\"] = write_relationship(\n",
    "    has_account_df, \"HAS_ACCOUNT\",\n",
    "    \"Customer\", \"customer_id\",\n",
    "    \"Account\", \"account_id\"\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# 2. AT_BANK: Account -> Bank\n",
    "print(\"[2/7] AT_BANK RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "at_bank_df = accounts.select(\"account_id\", \"bank_id\")\n",
    "rel_results[\"AT_BANK\"] = write_relationship(\n",
    "    at_bank_df, \"AT_BANK\",\n",
    "    \"Account\", \"account_id\",\n",
    "    \"Bank\", \"bank_id\"\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# 3. OF_COMPANY: Stock -> Company\n",
    "print(\"[3/7] OF_COMPANY RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "of_company_df = stocks.select(\"stock_id\", \"company_id\")\n",
    "rel_results[\"OF_COMPANY\"] = write_relationship(\n",
    "    of_company_df, \"OF_COMPANY\",\n",
    "    \"Stock\", \"stock_id\",\n",
    "    \"Company\", \"company_id\"\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# 4. PERFORMS: Account -> Transaction (from_account initiates)\n",
    "print(\"[4/7] PERFORMS RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"[DEBUG] Aliasing from_account_id -> account_id\")\n",
    "performs_df = transactions.select(\n",
    "    F.col(\"from_account_id\").alias(\"account_id\"),\n",
    "    \"transaction_id\"\n",
    ")\n",
    "rel_results[\"PERFORMS\"] = write_relationship(\n",
    "    performs_df, \"PERFORMS\",\n",
    "    \"Account\", \"account_id\",\n",
    "    \"Transaction\", \"transaction_id\"\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# 5. BENEFITS_TO: Transaction -> Account (to_account receives)\n",
    "print(\"[5/7] BENEFITS_TO RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"[DEBUG] Aliasing to_account_id -> account_id\")\n",
    "benefits_df = transactions.select(\n",
    "    \"transaction_id\",\n",
    "    F.col(\"to_account_id\").alias(\"account_id\")\n",
    ")\n",
    "rel_results[\"BENEFITS_TO\"] = write_relationship(\n",
    "    benefits_df, \"BENEFITS_TO\",\n",
    "    \"Transaction\", \"transaction_id\",\n",
    "    \"Account\", \"account_id\"\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# 6. HAS_POSITION: Account -> Position\n",
    "print(\"[6/7] HAS_POSITION RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "has_position_df = positions.select(\"account_id\", \"position_id\")\n",
    "rel_results[\"HAS_POSITION\"] = write_relationship(\n",
    "    has_position_df, \"HAS_POSITION\",\n",
    "    \"Account\", \"account_id\",\n",
    "    \"Position\", \"position_id\"\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# 7. OF_SECURITY: Position -> Stock\n",
    "print(\"[7/7] OF_SECURITY RELATIONSHIPS\")\n",
    "print(\"-\" * 40)\n",
    "of_security_df = positions.select(\"position_id\", \"stock_id\")\n",
    "rel_results[\"OF_SECURITY\"] = write_relationship(\n",
    "    of_security_df, \"OF_SECURITY\",\n",
    "    \"Position\", \"position_id\",\n",
    "    \"Stock\", \"stock_id\"\n",
    ")\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"RELATIONSHIP CREATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "print(f\"{'Type':<20} {'Count':>10} {'Time':>10} {'Status':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for rel_type, result in rel_results.items():\n",
    "    print(f\"{rel_type:<20} {result['count']:>10} {result['elapsed']:>9.2f}s {'[OK]':>10}\")\n",
    "print(\"-\" * 55)\n",
    "total_rels = sum(r['count'] for r in rel_results.values())\n",
    "print(f\"{'TOTAL':<20} {total_rels:>10} {total_elapsed:>9.2f}s\")\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"[OK] ALL RELATIONSHIPS WRITTEN SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Validate Import\n",
    "\n",
    "Run validation queries to verify the import completed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATE NODE COUNTS\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION - Node counts\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "# Expected counts\n",
    "expected_nodes = {\n",
    "    \"Customer\": 102,\n",
    "    \"Bank\": 102,\n",
    "    \"Account\": 123,\n",
    "    \"Company\": 102,\n",
    "    \"Stock\": 102,\n",
    "    \"Position\": 110,\n",
    "    \"Transaction\": 123\n",
    "}\n",
    "\n",
    "# Query actual counts\n",
    "print(\"[DEBUG] Running node count query...\")\n",
    "node_count_query = \"\"\"\n",
    "MATCH (n)\n",
    "RETURN labels(n)[0] AS label, count(n) AS count\n",
    "ORDER BY label\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    node_counts = run_cypher(node_count_query).collect()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  [OK] Query completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(f\"{'Label':<15} {'Expected':>10} {'Actual':>10} {'Status':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    all_valid = True\n",
    "    for row in node_counts:\n",
    "        label = row[\"label\"]\n",
    "        actual = row[\"count\"]\n",
    "        expected = expected_nodes.get(label, \"N/A\")\n",
    "        if expected == \"N/A\":\n",
    "            status = \"[?]\"\n",
    "        elif actual == expected:\n",
    "            status = \"[OK]\"\n",
    "        else:\n",
    "            status = \"[MISMATCH]\"\n",
    "            all_valid = False\n",
    "        print(f\"{label:<15} {expected:>10} {actual:>10} {status:>10}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    if all_valid:\n",
    "        print(\"\\n[OK] Node validation PASSED!\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Node validation FAILED - check mismatched counts\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] Validation query failed: {type(e).__name__}\")\n",
    "    print(f\"         {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATE RELATIONSHIP COUNTS\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION - Relationship counts\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "# Expected counts\n",
    "expected_rels = {\n",
    "    \"HAS_ACCOUNT\": 123,\n",
    "    \"AT_BANK\": 123,\n",
    "    \"OF_COMPANY\": 102,\n",
    "    \"PERFORMS\": 123,\n",
    "    \"BENEFITS_TO\": 123,\n",
    "    \"HAS_POSITION\": 110,\n",
    "    \"OF_SECURITY\": 110\n",
    "}\n",
    "\n",
    "# Query actual counts\n",
    "print(\"[DEBUG] Running relationship count query...\")\n",
    "rel_count_query = \"\"\"\n",
    "MATCH ()-[r]->()\n",
    "RETURN type(r) AS relationship_type, count(r) AS count\n",
    "ORDER BY relationship_type\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    rel_counts = run_cypher(rel_count_query).collect()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  [OK] Query completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(f\"{'Relationship':<20} {'Expected':>10} {'Actual':>10} {'Status':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    all_valid = True\n",
    "    for row in rel_counts:\n",
    "        rel_type = row[\"relationship_type\"]\n",
    "        actual = row[\"count\"]\n",
    "        expected = expected_rels.get(rel_type, \"N/A\")\n",
    "        if expected == \"N/A\":\n",
    "            status = \"[?]\"\n",
    "        elif actual == expected:\n",
    "            status = \"[OK]\"\n",
    "        else:\n",
    "            status = \"[MISMATCH]\"\n",
    "            all_valid = False\n",
    "        print(f\"{rel_type:<20} {expected:>10} {actual:>10} {status:>10}\")\n",
    "    \n",
    "    print(\"-\" * 55)\n",
    "    if all_valid:\n",
    "        print(\"\\n[OK] Relationship validation PASSED!\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Relationship validation FAILED - check mismatched counts\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] Validation query failed: {type(e).__name__}\")\n",
    "    print(f\"         {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE QUERIES - Customer Profile\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE QUERY 1 - Customer C0001's complete financial profile\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "query1 = \"\"\"\n",
    "MATCH (c:Customer {customer_id: 'C0001'})-[:HAS_ACCOUNT]->(a:Account)\n",
    "OPTIONAL MATCH (a)-[:AT_BANK]->(b:Bank)\n",
    "OPTIONAL MATCH (a)-[:HAS_POSITION]->(p:Position)-[:OF_SECURITY]->(s:Stock)\n",
    "RETURN\n",
    "    c.first_name + ' ' + c.last_name AS customer_name,\n",
    "    a.account_id AS account,\n",
    "    a.account_type AS account_type,\n",
    "    a.balance AS balance,\n",
    "    b.name AS bank_name,\n",
    "    s.ticker AS ticker,\n",
    "    p.shares AS shares,\n",
    "    p.current_value AS holding_value\n",
    "ORDER BY holding_value DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"[DEBUG] Query:\")\n",
    "for line in query1.strip().split('\\n'):\n",
    "    print(f\"  {line}\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    result = run_cypher(query1)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[OK] Query completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    display(result)\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Query failed: {type(e).__name__}\")\n",
    "    print(f\"       {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE QUERIES - Top Banks by Deposits\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE QUERY 2 - Top 5 banks by total deposits\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "query2 = \"\"\"\n",
    "MATCH (b:Bank)<-[:AT_BANK]-(a:Account)\n",
    "RETURN\n",
    "    b.name AS bank_name,\n",
    "    b.bank_type AS bank_type,\n",
    "    count(DISTINCT a) AS num_accounts,\n",
    "    round(sum(a.balance), 2) AS total_deposits\n",
    "ORDER BY total_deposits DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"[DEBUG] Query:\")\n",
    "for line in query2.strip().split('\\n'):\n",
    "    print(f\"  {line}\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    result = run_cypher(query2)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[OK] Query completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    display(result)\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Query failed: {type(e).__name__}\")\n",
    "    print(f\"       {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE QUERIES - Transaction Flow\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE QUERY 3 - Recent transaction flow\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "query3 = \"\"\"\n",
    "MATCH (from:Account)-[:PERFORMS]->(t:Transaction)-[:BENEFITS_TO]->(to:Account)\n",
    "RETURN\n",
    "    from.account_id AS from_account,\n",
    "    t.transaction_id AS transaction_id,\n",
    "    t.amount AS amount,\n",
    "    t.transaction_date AS date,\n",
    "    to.account_id AS to_account\n",
    "ORDER BY t.transaction_date DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"[DEBUG] Query:\")\n",
    "for line in query3.strip().split('\\n'):\n",
    "    print(f\"  {line}\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    result = run_cypher(query3)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[OK] Query completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    display(result)\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Query failed: {type(e).__name__}\")\n",
    "    print(f\"       {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE QUERIES - Most Popular Stocks\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE QUERY 4 - Most widely held stocks\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "query4 = \"\"\"\n",
    "MATCH (a:Account)-[:HAS_POSITION]->(p:Position)-[:OF_SECURITY]->(s:Stock)-[:OF_COMPANY]->(c:Company)\n",
    "RETURN\n",
    "    c.name AS company_name,\n",
    "    s.ticker AS ticker,\n",
    "    count(DISTINCT a) AS num_holders,\n",
    "    sum(p.shares) AS total_shares_held,\n",
    "    round(sum(p.current_value), 2) AS total_market_value\n",
    "ORDER BY num_holders DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"[DEBUG] Query:\")\n",
    "for line in query4.strip().split('\\n'):\n",
    "    print(f\"  {line}\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    result = run_cypher(query4)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[OK] Query completed in {elapsed:.2f}s\")\n",
    "    print(\"\")\n",
    "    display(result)\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Query failed: {type(e).__name__}\")\n",
    "    print(f\"       {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Complete\n",
    "\n",
    "The financial demo data has been successfully imported to Neo4j!\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Nodes Created:**\n",
    "- 102 Customers\n",
    "- 102 Banks\n",
    "- 123 Accounts\n",
    "- 102 Companies\n",
    "- 102 Stocks\n",
    "- 110 Positions\n",
    "- 123 Transactions\n",
    "\n",
    "**Relationships Created:**\n",
    "- 123 HAS_ACCOUNT (Customer -> Account)\n",
    "- 123 AT_BANK (Account -> Bank)\n",
    "- 102 OF_COMPANY (Stock -> Company)\n",
    "- 123 PERFORMS (Account -> Transaction)\n",
    "- 123 BENEFITS_TO (Transaction -> Account)\n",
    "- 110 HAS_POSITION (Account -> Position)\n",
    "- 110 OF_SECURITY (Position -> Stock)\n",
    "\n",
    "### Debug Logging\n",
    "\n",
    "This notebook includes `[DEBUG]`, `[OK]`, and `[FAIL]` markers throughout to help identify issues:\n",
    "\n",
    "- `[DEBUG]` - Informational messages about what operation is being attempted\n",
    "- `[OK]` - Operation completed successfully\n",
    "- `[FAIL]` - Operation failed (see error details)\n",
    "- `[WARNING]` - Non-fatal issue that may need attention\n",
    "- `[SKIP]` - Operation skipped (e.g., constraint already exists)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Explore with Neo4j Browser** - Visualize the graph\n",
    "2. **Run Graph Algorithms** - PageRank, Community Detection\n",
    "3. **Build Applications** - Portfolio dashboards, risk analysis\n",
    "4. **Extend the Schema** - Add Advisors, Branches, Products\n",
    "\n",
    "---\n",
    "\n",
    "See `IMPORT.md` for setup instructions and troubleshooting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}