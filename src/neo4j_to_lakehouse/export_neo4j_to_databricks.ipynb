{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Neo4j Graph Data to Databricks Unity Catalog\n",
    "\n",
    "This notebook extracts nodes and relationships from Neo4j and writes them as Delta tables in Unity Catalog.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Neo4j database** with financial demo data loaded\n",
    "2. **Databricks Secrets** - run `./scripts/setup_databricks_secrets.sh`\n",
    "3. **Databricks cluster** with Neo4j Spark Connector installed\n",
    "4. **Cluster access mode**: Dedicated (not Shared)\n",
    "\n",
    "## Output\n",
    "\n",
    "Creates 14 Delta tables in Unity Catalog:\n",
    "- 7 node tables: customer, bank, account, company, stock, position, transaction\n",
    "- 7 relationship tables: has_account, at_bank, of_company, performs, benefits_to, has_position, of_security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Default catalog/schema (will be overridden from secrets)\n",
    "CATALOG = \"neo4j_augmentation_demo\"\n",
    "SCHEMA = \"graph_data\"\n",
    "\n",
    "# Node labels to extract\n",
    "NODE_LABELS = [\n",
    "    \"Customer\",\n",
    "    \"Bank\",\n",
    "    \"Account\",\n",
    "    \"Company\",\n",
    "    \"Stock\",\n",
    "    \"Position\",\n",
    "    \"Transaction\",\n",
    "]\n",
    "\n",
    "# Relationships: (type, source_label, target_label)\n",
    "RELATIONSHIPS = [\n",
    "    (\"HAS_ACCOUNT\", \"Customer\", \"Account\"),\n",
    "    (\"AT_BANK\", \"Account\", \"Bank\"),\n",
    "    (\"OF_COMPANY\", \"Stock\", \"Company\"),\n",
    "    (\"PERFORMS\", \"Account\", \"Transaction\"),\n",
    "    (\"BENEFITS_TO\", \"Transaction\", \"Account\"),\n",
    "    (\"HAS_POSITION\", \"Account\", \"Position\"),\n",
    "    (\"OF_SECURITY\", \"Position\", \"Stock\"),\n",
    "]\n",
    "\n",
    "print(f\"Nodes to export: {len(NODE_LABELS)}\")\n",
    "print(f\"Relationships to export: {len(RELATIONSHIPS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading credentials from Databricks Secrets...\")\n",
    "\n",
    "NEO4J_URL = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"url\")\n",
    "NEO4J_USER = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"username\")\n",
    "NEO4J_PASS = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"password\")\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Extract catalog/schema from volume_path: /Volumes/{catalog}/{schema}/{volume}\n",
    "try:\n",
    "    volume_path = dbutils.secrets.get(scope=\"neo4j-creds\", key=\"volume_path\")\n",
    "    parts = volume_path.strip(\"/\").split(\"/\")\n",
    "    if len(parts) >= 3 and parts[0] == \"Volumes\":\n",
    "        CATALOG = parts[1]\n",
    "        SCHEMA = parts[2]\n",
    "        print(f\"[OK] Catalog: {CATALOG}\")\n",
    "        print(f\"[OK] Schema: {SCHEMA}\")\n",
    "except Exception:\n",
    "    print(f\"[INFO] Using defaults: {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Configure Spark for Neo4j\n",
    "spark.conf.set(\"neo4j.url\", NEO4J_URL)\n",
    "spark.conf.set(\"neo4j.authentication.basic.username\", NEO4J_USER)\n",
    "spark.conf.set(\"neo4j.authentication.basic.password\", NEO4J_PASS)\n",
    "spark.conf.set(\"neo4j.database\", NEO4J_DATABASE)\n",
    "\n",
    "print(f\"[OK] Neo4j URL: {NEO4J_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Neo4j Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Neo4j connection...\")\n",
    "\n",
    "try:\n",
    "    test_df = spark.read.format(\"org.neo4j.spark.DataSource\").option(\"query\", \"RETURN 1 AS test\").load()\n",
    "    test_df.collect()\n",
    "    print(\"[OK] Connected to Neo4j!\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up Unity Catalog...\")\n",
    "\n",
    "# Check catalog exists\n",
    "try:\n",
    "    catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if CATALOG not in catalogs:\n",
    "        print(f\"[ERROR] Catalog '{CATALOG}' not found.\")\n",
    "        print(f\"Available: {catalogs}\")\n",
    "        raise ValueError(f\"Catalog '{CATALOG}' not found\")\n",
    "    print(f\"[OK] Catalog exists: {CATALOG}\")\n",
    "except ValueError:\n",
    "    raise\n",
    "except Exception:\n",
    "    print(f\"[INFO] Could not list catalogs, trying to use '{CATALOG}' directly\")\n",
    "\n",
    "# Create schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "print(f\"[OK] Schema ready: {CATALOG}.{SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nodes(label):\n",
    "    \"\"\"Read nodes from Neo4j.\"\"\"\n",
    "    return spark.read.format(\"org.neo4j.spark.DataSource\").option(\"labels\", label).load()\n",
    "\n",
    "def read_relationship(rel_type, source_label, target_label):\n",
    "    \"\"\"Read relationships from Neo4j.\"\"\"\n",
    "    return (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"relationship\", rel_type)\n",
    "        .option(\"relationship.source.labels\", source_label)\n",
    "        .option(\"relationship.target.labels\", target_label)\n",
    "        .option(\"relationship.nodes.map\", \"false\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "def write_table(df, table_name):\n",
    "    \"\"\"Write DataFrame to Delta table.\"\"\"\n",
    "    full_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    return df.count()\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING NODES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "node_results = {}\n",
    "\n",
    "for i, label in enumerate(NODE_LABELS, 1):\n",
    "    table_name = label.lower()\n",
    "    print(f\"\\n[{i}/{len(NODE_LABELS)}] {label} -> {table_name}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    df = read_nodes(label)\n",
    "    count = write_table(df, table_name)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    node_results[label] = {\"count\": count, \"time\": elapsed}\n",
    "    print(f\"    [OK] {count} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nExported {len(node_results)} node tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING RELATIONSHIPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rel_results = {}\n",
    "\n",
    "for i, (rel_type, source, target) in enumerate(RELATIONSHIPS, 1):\n",
    "    table_name = rel_type.lower()\n",
    "    print(f\"\\n[{i}/{len(RELATIONSHIPS)}] {rel_type} -> {table_name}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    df = read_relationship(rel_type, source, target)\n",
    "    count = write_table(df, table_name)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    rel_results[rel_type] = {\"count\": count, \"time\": elapsed}\n",
    "    print(f\"    [OK] {count} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nExported {len(rel_results)} relationship tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "expected_nodes = {\n",
    "    \"Customer\": 102, \"Bank\": 102, \"Account\": 123, \"Company\": 102,\n",
    "    \"Stock\": 102, \"Position\": 110, \"Transaction\": 123,\n",
    "}\n",
    "\n",
    "expected_rels = {\n",
    "    \"HAS_ACCOUNT\": 123, \"AT_BANK\": 123, \"OF_COMPANY\": 102, \"PERFORMS\": 123,\n",
    "    \"BENEFITS_TO\": 123, \"HAS_POSITION\": 110, \"OF_SECURITY\": 110,\n",
    "}\n",
    "\n",
    "all_valid = True\n",
    "\n",
    "print(f\"\\n{'Table':<15} {'Expected':>10} {'Actual':>10} {'Status':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label, expected in expected_nodes.items():\n",
    "    actual = node_results.get(label, {}).get(\"count\", 0)\n",
    "    status = \"OK\" if actual == expected else \"MISMATCH\"\n",
    "    if actual != expected: all_valid = False\n",
    "    print(f\"{label.lower():<15} {expected:>10} {actual:>10} {status:>10}\")\n",
    "\n",
    "for rel_type, expected in expected_rels.items():\n",
    "    actual = rel_results.get(rel_type, {}).get(\"count\", 0)\n",
    "    status = \"OK\" if actual == expected else \"MISMATCH\"\n",
    "    if actual != expected: all_valid = False\n",
    "    print(f\"{rel_type.lower():<15} {expected:>10} {actual:>10} {status:>10}\")\n",
    "\n",
    "print(f\"\\n{'All validations passed!' if all_valid else 'Some counts do not match'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = sum(r[\"count\"] for r in node_results.values())\n",
    "total_rels = sum(r[\"count\"] for r in rel_results.values())\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Destination: {CATALOG}.{SCHEMA}\")\n",
    "print(f\"Tables: {len(node_results) + len(rel_results)}\")\n",
    "print(f\"Total rows: {total_nodes + total_rels}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
